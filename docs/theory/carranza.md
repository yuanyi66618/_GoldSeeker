# Carranzaç†è®ºåŸºç¡€

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»Emmanuel John M. Carranzaçš„åœ°çƒåŒ–å­¦å¼‚å¸¸ä¸çŸ¿äº§è¿œæ™¯åˆ¶å›¾ç†è®ºï¼Œè¿™æ˜¯Gold-Seekerå¹³å°çš„æ ¸å¿ƒç†è®ºåŸºç¡€ã€‚

## ğŸ“š ç†è®ºæ¦‚è¿°

### Carranza (2009) æ ¸å¿ƒç†è®º

Carranzaåœ¨ã€ŠGeochemical Anomaly and Mineral Prospectivity Mapping in GISã€‹ä¸€ä¹¦ä¸­æå‡ºäº†ç³»ç»Ÿçš„åœ°çƒåŒ–å­¦æ‰¾çŸ¿é¢„æµ‹ç†è®ºæ¡†æ¶ï¼Œè¯¥ç†è®ºå·²æˆä¸ºç°ä»£åœ°çƒåŒ–å­¦å‹˜æ¢çš„æ ‡å‡†æ–¹æ³•ã€‚

#### ç†è®ºæ ¸å¿ƒè¦ç´ 

1. **åœ°çƒåŒ–å­¦å¼‚å¸¸è¯†åˆ«**: åŸºäºç»Ÿè®¡å­¦å’Œåˆ†å½¢ç†è®ºçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•
2. **è¯æ®å›¾å±‚æ„å»º**: å°†åœ°çƒåŒ–å­¦æ•°æ®è½¬æ¢ä¸ºæ‰¾çŸ¿è¯æ®
3. **è¯æ®æƒæ–¹æ³•**: å®šé‡è¯„ä¼°å„è¯æ®å›¾å±‚çš„é‡è¦æ€§
4. **ç©ºé—´åˆ†æ**: ç»“åˆåœ°è´¨ã€åœ°çƒåŒ–å­¦å’Œé¥æ„Ÿæ•°æ®è¿›è¡Œç»¼åˆåˆ†æ
5. **GISé›†æˆ**: åœ¨åœ°ç†ä¿¡æ¯ç³»ç»Ÿä¸­è¿›è¡Œç©ºé—´å»ºæ¨¡å’Œå¯è§†åŒ–

### ç†è®ºå‘å±•å†ç¨‹

```
ä¼ ç»Ÿåœ°çƒåŒ–å­¦å‹˜æ¢ â†’ ç»Ÿè®¡å­¦æ–¹æ³• â†’ åˆ†å½¢ç†è®º â†’ GISé›†æˆ â†’ å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
     (1970s)           (1980s)      (1990s)      (2000s)        (2020s)
```

## ğŸ”¬ åœ°çƒåŒ–å­¦å¼‚å¸¸ç†è®º

### å¼‚å¸¸å®šä¹‰

#### èƒŒæ™¯ä¸å¼‚å¸¸
```python
# èƒŒæ™¯ä¸å¼‚å¸¸çš„æ•°å­¦å®šä¹‰
def classify_anomaly(data, threshold):
    """
    æ ¹æ®é˜ˆå€¼åˆ†ç±»èƒŒæ™¯å’Œå¼‚å¸¸
    
    å‚æ•°:
    - data: åœ°çƒåŒ–å­¦æ•°æ®
    - threshold: å¼‚å¸¸é˜ˆå€¼
    
    è¿”å›:
    - background: èƒŒæ™¯å€¼
    - anomaly: å¼‚å¸¸å€¼
    """
    background = data[data <= threshold]
    anomaly = data[data > threshold]
    return background, anomaly
```

#### å¼‚å¸¸ç±»å‹

1. **å±€éƒ¨å¼‚å¸¸**: å±€éƒ¨é«˜å€¼åŒºåŸŸï¼Œé€šå¸¸ä¸çŸ¿åŒ–ç›´æ¥ç›¸å…³
2. **åŒºåŸŸå¼‚å¸¸**: å¤§èŒƒå›´çš„é«˜å€¼åŒºåŸŸï¼Œå¯èƒ½åæ˜ åœ°è´¨æ„é€ 
3. **å¤šé‡å¼‚å¸¸**: å¤šä¸ªå¼‚å¸¸åŒºåŸŸçš„ç»„åˆï¼ŒæŒ‡ç¤ºå¤æ‚çš„æˆçŸ¿è¿‡ç¨‹

### ç»Ÿè®¡å­¦å¼‚å¸¸è¯†åˆ«

#### ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•

```python
class StatisticalAnomalyDetector:
    """åŸºäºç»Ÿè®¡å­¦çš„å¼‚å¸¸æ£€æµ‹"""
    
    def mean_plus_2sd(self, data):
        """å‡å€¼+2å€æ ‡å‡†å·®æ³•"""
        mean = np.mean(data)
        std = np.std(data)
        threshold = mean + 2 * std
        return threshold
    
    def percentile_method(self, data, percentile=95):
        """ç™¾åˆ†ä½æ•°æ³•"""
        threshold = np.percentile(data, percentile)
        return threshold
    
    def boxplot_method(self, data):
        """ç®±çº¿å›¾æ³•"""
        q1 = np.percentile(data, 25)
        q3 = np.percentile(data, 75)
        iqr = q3 - q1
        threshold = q3 + 1.5 * iqr
        return threshold
```

#### å¤šå…ƒç»Ÿè®¡æ–¹æ³•

```python
class MultivariateAnomalyDetector:
    """å¤šå…ƒç»Ÿè®¡å¼‚å¸¸æ£€æµ‹"""
    
    def mahalanobis_distance(self, data, cov_inv=None):
        """é©¬æ°è·ç¦»æ³•"""
        if cov_inv is None:
            cov = np.cov(data.T)
            cov_inv = np.linalg.inv(cov)
        
        mean = np.mean(data, axis=0)
        diff = data - mean
        mahal_dist = np.sqrt(np.sum(diff @ cov_inv * diff, axis=1))
        return mahal_dist
    
    def principal_component_analysis(self, data, n_components=2):
        """ä¸»æˆåˆ†åˆ†æ"""
        from sklearn.decomposition import PCA
        pca = PCA(n_components=n_components)
        transformed = pca.fit_transform(data)
        return transformed, pca.explained_variance_ratio_
```

## ğŸŒŠ åˆ†å½¢å¼‚å¸¸ç†è®º

### åˆ†å½¢ç†è®ºåŸºç¡€

#### C-Aåˆ†å½¢æ¨¡å‹

Cheng, Agterbergå’ŒBallantyne (1994) æå‡ºçš„æµ“åº¦-é¢ç§¯ï¼ˆConcentration-Area, C-Aï¼‰åˆ†å½¢æ¨¡å‹æ˜¯Carranzaç†è®ºçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

```python
class FractalAnomalyDetector:
    """åŸºäºåˆ†å½¢ç†è®ºçš„å¼‚å¸¸æ£€æµ‹"""
    
    def calculate_ca_relationship(self, data, bins=100):
        """è®¡ç®—C-Aå…³ç³»"""
        # åˆ›å»ºæµ“åº¦åŒºé—´
        concentrations = np.linspace(data.min(), data.max(), bins)
        areas = []
        
        for c in concentrations:
            area = np.sum(data >= c)
            areas.append(area)
        
        return concentrations, areas
    
    def plot_ca_loglog(self, concentrations, areas):
        """ç»˜åˆ¶C-AåŒå¯¹æ•°å›¾"""
        log_c = np.log10(concentrations[concentrations > 0])
        log_a = np.log10(areas[concentrations > 0])
        
        plt.figure(figsize=(10, 6))
        plt.plot(log_c, log_a, 'b-', linewidth=2)
        plt.xlabel('log(æµ“åº¦)')
        plt.ylabel('log(é¢ç§¯)')
        plt.title('C-Aåˆ†å½¢å…³ç³»')
        plt.grid(True)
        plt.show()
        
        return log_c, log_a
    
    def detect_fractal_breaks(self, log_c, log_a):
        """æ£€æµ‹åˆ†å½¢æ–­ç‚¹"""
        # ä½¿ç”¨ä¸€é˜¶å¯¼æ•°æ£€æµ‹æ–­ç‚¹
        derivatives = np.diff(log_a) / np.diff(log_c)
        
        # å¯»æ‰¾å¯¼æ•°çš„æå€¼ç‚¹
        from scipy.signal import find_peaks
        peaks, _ = find_peaks(-np.abs(derivatives), distance=5)
        
        return peaks, derivatives
```

#### åˆ†å½¢ç»´æ•°è®¡ç®—

```python
def calculate_fractal_dimension(log_c, log_a, start_idx, end_idx):
    """è®¡ç®—åˆ†å½¢ç»´æ•°"""
    # çº¿æ€§å›å½’æ‹Ÿåˆ
    x = log_c[start_idx:end_idx]
    y = log_a[start_idx:end_idx]
    
    coeffs = np.polyfit(x, y, 1)
    slope = coeffs[0]
    
    # åˆ†å½¢ç»´æ•°æ˜¯æ–œç‡çš„è´Ÿå€¼
    fractal_dimension = -slope
    
    return fractal_dimension, coeffs
```

### å¤šé‡åˆ†å½¢åˆ†æ

#### å¤šé‡åˆ†å½¢è°±

```python
class MultifractalAnalysis:
    """å¤šé‡åˆ†å½¢åˆ†æ"""
    
    def calculate_multifractal_spectrum(self, data, q_values=None):
        """è®¡ç®—å¤šé‡åˆ†å½¢è°±"""
        if q_values is None:
            q_values = np.linspace(-5, 5, 21)
        
        tau_q = []
        alpha_q = []
        f_alpha = []
        
        for q in q_values:
            # è®¡ç®—é…åˆ†å‡½æ•°
            partition = self.calculate_partition_function(data, q)
            tau_q.append(np.log(partition))
            
            # è®¡ç®—å¥‡å¼‚æŒ‡æ•°
            alpha = self.calculate_singularity_exponent(data, q)
            alpha_q.append(alpha)
            
            # è®¡ç®—å¤šé‡åˆ†å½¢è°±
            f_alpha.append(q * alpha - tau_q[-1])
        
        return q_values, tau_q, alpha_q, f_alpha
    
    def calculate_partition_function(self, data, q, scales=None):
        """è®¡ç®—é…åˆ†å‡½æ•°"""
        if scales is None:
            scales = [2, 4, 8, 16, 32]
        
        partition_values = []
        
        for scale in scales:
            # å°†æ•°æ®åˆ†å‰²ä¸ºå°ºåº¦ä¸ºscaleçš„ç›’å­
            boxes = self.partition_data(data, scale)
            
            # è®¡ç®—æ¯ä¸ªç›’å­çš„æ¦‚ç‡
            probabilities = [np.sum(box) / np.sum(data) for box in boxes]
            
            # è®¡ç®—é…åˆ†å‡½æ•°
            partition = np.sum([p**q for p in probabilities if p > 0])
            partition_values.append(partition)
        
        return partition_values
```

## âš–ï¸ è¯æ®æƒæ–¹æ³•

### è¯æ®æƒç†è®ºåŸºç¡€

#### åŸºæœ¬æ¦‚å¿µ

è¯æ®æƒæ–¹æ³•ï¼ˆWeights of Evidence, WofEï¼‰æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„å®šé‡é¢„æµ‹æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å„ç§è¯æ®å›¾å±‚å¯¹ç›®æ ‡çŸ¿åºŠçš„æŒ‡ç¤ºä½œç”¨ã€‚

```python
class WeightsOfEvidence:
    """è¯æ®æƒæ–¹æ³•å®ç°"""
    
    def calculate_weights(self, evidence_map, target_map):
        """è®¡ç®—è¯æ®æƒ"""
        # è®¡ç®—å„ç§ç»Ÿè®¡é‡
        total_area = np.prod(evidence_map.shape)
        target_area = np.sum(target_map > 0)
        
        # è®¡ç®—W+ï¼ˆè¯æ®å­˜åœ¨æ—¶çš„æƒé‡ï¼‰
        evidence_with_target = np.sum((evidence_map > 0) & (target_map > 0))
        evidence_without_target = np.sum((evidence_map > 0) & (target_map == 0))
        
        w_plus = np.log((evidence_with_target / target_area) / 
                       (evidence_without_target / (total_area - target_area)))
        
        # è®¡ç®—W-ï¼ˆè¯æ®ä¸å­˜åœ¨æ—¶çš„æƒé‡ï¼‰
        no_evidence_with_target = np.sum((evidence_map == 0) & (target_map > 0))
        no_evidence_without_target = np.sum((evidence_map == 0) & (target_map == 0))
        
        w_minus = np.log((no_evidence_with_target / target_area) / 
                        (no_evidence_without_target / (total_area - target_area)))
        
        # è®¡ç®—å¯¹æ¯”åº¦
        contrast = w_plus - w_minus
        
        return {
            'w_plus': w_plus,
            'w_minus': w_minus,
            'contrast': contrast
        }
    
    def calculate_studentized_contrast(self, w_plus, w_minus, n_plus, n_minus):
        """è®¡ç®—å­¦ç”ŸåŒ–å¯¹æ¯”åº¦"""
        contrast = w_plus - w_minus
        
        # è®¡ç®—æ–¹å·®
        var_w_plus = 1 / n_plus + 1 / n_minus
        var_w_minus = 1 / n_plus + 1 / n_minus
        
        # è®¡ç®—å­¦ç”ŸåŒ–å¯¹æ¯”åº¦
        studentized_contrast = contrast / np.sqrt(var_w_plus + var_w_minus)
        
        return studentized_contrast
```

#### è¯æ®ç»„åˆ

```python
def combine_evidence(weights_list):
    """ç»„åˆå¤šä¸ªè¯æ®çš„æƒé‡"""
    total_w_plus = sum([w['w_plus'] for w in weights_list])
    total_w_minus = sum([w['w_minus'] for w in weights_list])
    total_contrast = sum([w['contrast'] for w in weights_list])
    
    return {
        'total_w_plus': total_w_plus,
        'total_w_minus': total_w_minus,
        'total_contrast': total_contrast
    }
```

### æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ

#### å¡æ–¹æ£€éªŒ

```python
def chi_square_test(evidence1, evidence2, target):
    """å¡æ–¹æ£€éªŒæ¡ä»¶ç‹¬ç«‹æ€§"""
    # åˆ›å»ºåˆ—è”è¡¨
    contingency_table = np.zeros((2, 2, 2))
    
    # å¡«å……åˆ—è”è¡¨
    for i in [0, 1]:  # evidence1
        for j in [0, 1]:  # evidence2
            for k in [0, 1]:  # target
                mask = (evidence1 == i) & (evidence2 == j) & (target == k)
                contingency_table[i, j, k] = np.sum(mask)
    
    # æ‰§è¡Œå¡æ–¹æ£€éªŒ
    from scipy.stats import chi2_contingency
    chi2, p_value, dof, expected = chi2_contingency(contingency_table.reshape(4, 2))
    
    return chi2, p_value, dof
```

## ğŸ—ºï¸ ç©ºé—´åˆ†æç†è®º

### ç©ºé—´è‡ªç›¸å…³

#### Moran's I

```python
def calculate_morans_i(data, weights_matrix):
    """è®¡ç®—Moran's Iç©ºé—´è‡ªç›¸å…³æŒ‡æ•°"""
    n = len(data)
    
    # è®¡ç®—å‡å€¼
    mean_data = np.mean(data)
    
    # è®¡ç®—åˆ†å­
    numerator = 0
    for i in range(n):
        for j in range(n):
            numerator += weights_matrix[i, j] * (data[i] - mean_data) * (data[j] - mean_data)
    
    # è®¡ç®—åˆ†æ¯
    denominator = np.sum((data - mean_data) ** 2)
    
    # è®¡ç®—æƒé‡æ€»å’Œ
    sum_weights = np.sum(weights_matrix)
    
    # è®¡ç®—Moran's I
    morans_i = (n / sum_weights) * (numerator / denominator)
    
    return morans_i
```

#### Getis-Ord G*

```python
def calculate_getis_ord_g(data, coordinates, distance_threshold):
    """è®¡ç®—Getis-Ord G*ç»Ÿè®¡é‡"""
    n = len(data)
    g_star_values = np.zeros(n)
    
    # è®¡ç®—è·ç¦»çŸ©é˜µ
    from scipy.spatial.distance import cdist
    distance_matrix = cdist(coordinates, coordinates)
    
    # åˆ›å»ºæƒé‡çŸ©é˜µ
    weights_matrix = (distance_matrix <= distance_threshold).astype(float)
    np.fill_diagonal(weights_matrix, 0)  # æ’é™¤è‡ªèº«
    
    for i in range(n):
        # è·å–é‚»å±…
        neighbors = np.where(weights_matrix[i] > 0)[0]
        
        if len(neighbors) > 0:
            # è®¡ç®—G*å€¼
            sum_data = np.sum(data[neighbors])
            sum_weights = np.sum(weights_matrix[i, neighbors])
            
            g_star_values[i] = sum_data / sum_weights
    
    return g_star_values
```

### ç©ºé—´æ’å€¼

#### å…‹é‡Œé‡‘æ’å€¼

```python
class KrigingInterpolator:
    """å…‹é‡Œé‡‘æ’å€¼å™¨"""
    
    def __init__(self, variogram_model='spherical'):
        self.variogram_model = variogram_model
        self.fitted_model = None
    
    def fit_variogram(self, coordinates, values):
        """æ‹Ÿåˆå˜å¼‚å‡½æ•°"""
        from pykrige.ok import OrdinaryKriging
        
        # åˆ›å»ºæ™®é€šå…‹é‡Œé‡‘æ¨¡å‹
        ok = OrdinaryKriging(
            coordinates[:, 0], coordinates[:, 1], values,
            variogram_model=self.variogram_model,
            verbose=False
        )
        
        self.fitted_model = ok
        return ok
    
    def interpolate(self, grid_x, grid_y):
        """æ‰§è¡Œæ’å€¼"""
        if self.fitted_model is None:
            raise ValueError("å¿…é¡»å…ˆæ‹Ÿåˆå˜å¼‚å‡½æ•°")
        
        z, ss = self.fitted_model.execute('grid', grid_x, grid_y)
        return z, ss
```

## ğŸ“Š æ•°æ®å¤„ç†ç†è®º

### æ•°æ®é¢„å¤„ç†

#### åˆ å¤±æ•°æ®å¤„ç†

```python
class CensoredDataProcessor:
    """åˆ å¤±æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, detection_limits):
        self.detection_limits = detection_limits
    
    def impute_censored_data(self, data, method='dl_over_2'):
        """å¤„ç†åˆ å¤±æ•°æ®"""
        imputed_data = data.copy()
        
        for element, dl in self.detection_limits.items():
            if element in data.columns:
                censored_mask = data[element] < dl
                
                if method == 'dl_over_2':
                    # æ£€æµ‹é™/2æ–¹æ³•
                    imputed_data.loc[censored_mask, element] = dl / 2
                
                elif method == 'rosner':
                    # Rosneræ–¹æ³•
                    imputed_data.loc[censored_mask, element] = self._rosner_imputation(
                        data[element], censored_mask, dl
                    )
                
                elif method == 'maximum_likelihood':
                    # æœ€å¤§ä¼¼ç„¶ä¼°è®¡
                    imputed_data.loc[censored_mask, element] = self._ml_imputation(
                        data[element], censored_mask, dl
                    )
        
        return imputed_data
    
    def _rosner_imputation(self, data, censored_mask, dl):
        """Rosneråˆ å¤±æ•°æ®æ’è¡¥"""
        from scipy import stats
        
        # ä½¿ç”¨éåˆ å¤±æ•°æ®ä¼°è®¡å‚æ•°
        uncensored_data = data[~censored_mask]
        mean, std = stats.norm.fit(uncensored_data)
        
        # è®¡ç®—åˆ å¤±æ•°æ®çš„æœŸæœ›å€¼
        from scipy.stats import norm
        z_dl = (dl - mean) / std
        expected_value = mean - std * norm.pdf(z_dl) / norm.cdf(z_dl)
        
        return expected_value
```

#### æ•°æ®è½¬æ¢

```python
class DataTransformer:
    """æ•°æ®è½¬æ¢å™¨"""
    
    def clr_transform(self, data):
        """ä¸­å¿ƒå¯¹æ•°æ¯”è½¬æ¢"""
        # æ·»åŠ å°å¸¸æ•°é¿å…å¯¹æ•°é›¶
        epsilon = 1e-10
        data = data + epsilon
        
        # è®¡ç®—å‡ ä½•å‡å€¼
        geometric_mean = np.exp(np.mean(np.log(data), axis=1))
        
        # CLRè½¬æ¢
        clr_data = np.log(data.values / geometric_mean[:, np.newaxis])
        
        return clr_data
    
    def alr_transform(self, data, reference_column):
        """åŠ æ³•å¯¹æ•°æ¯”è½¬æ¢"""
        reference_data = data[reference_column].values
        alr_data = np.log(data.drop(columns=[reference_column]).values / reference_data[:, np.newaxis])
        
        return alr_data
    
    def ilr_transform(self, data):
        """ç­‰è·å¯¹æ•°æ¯”è½¬æ¢"""
        from skbio.stats.composition import ilr
        ilr_data = ilr(data.values)
        
        return ilr_data
```

### å¼‚å¸¸å€¼æ£€æµ‹

#### å¤šå…ƒå¼‚å¸¸å€¼æ£€æµ‹

```python
class MultivariateOutlierDetector:
    """å¤šå…ƒå¼‚å¸¸å€¼æ£€æµ‹å™¨"""
    
    def __init__(self, method='mahalanobis'):
        self.method = method
    
    def detect_outliers(self, data):
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        if self.method == 'mahalanobis':
            return self._mahalanobis_detection(data)
        elif self.method == 'robust_mahalanobis':
            return self._robust_mahalanobis_detection(data)
        elif self.method == 'isolation_forest':
            return self._isolation_forest_detection(data)
        else:
            raise ValueError(f"æœªçŸ¥æ–¹æ³•: {self.method}")
    
    def _mahalanobis_detection(self, data):
        """é©¬æ°è·ç¦»å¼‚å¸¸å€¼æ£€æµ‹"""
        from scipy.stats import chi2
        
        # è®¡ç®—é©¬æ°è·ç¦»
        cov_matrix = np.cov(data.T)
        inv_cov_matrix = np.linalg.inv(cov_matrix)
        mean_vector = np.mean(data, axis=0)
        
        mahal_distances = []
        for i in range(len(data)):
            diff = data[i] - mean_vector
            mahal_dist = np.sqrt(diff @ inv_cov_matrix @ diff.T)
            mahal_distances.append(mahal_dist)
        
        # è®¡ç®—é˜ˆå€¼
        threshold = chi2.ppf(0.975, df=data.shape[1])
        
        # æ ‡è¯†å¼‚å¸¸å€¼
        outliers = np.array(mahal_distances) > np.sqrt(threshold)
        
        return outliers, mahal_distances
```

## ğŸ¯ æ‰¾çŸ¿é¢„æµ‹æ¨¡å‹

### è¯æ®å›¾å±‚æ„å»º

#### äºŒå€¼è¯æ®å›¾å±‚

```python
class BinaryEvidenceLayer:
    """äºŒå€¼è¯æ®å›¾å±‚æ„å»ºå™¨"""
    
    def create_binary_layer(self, continuous_data, threshold, operator='>'):
        """åˆ›å»ºäºŒå€¼è¯æ®å›¾å±‚"""
        if operator == '>':
            binary_layer = (continuous_data > threshold).astype(int)
        elif operator == '<':
            binary_layer = (continuous_data < threshold).astype(int)
        elif operator == '>=':
            binary_layer = (continuous_data >= threshold).astype(int)
        elif operator == '<=':
            binary_layer = (continuous_data <= threshold).astype(int)
        else:
            raise ValueError(f"æœªçŸ¥æ“ä½œç¬¦: {operator}")
        
        return binary_layer
    
    def optimize_threshold(self, evidence_data, target_data):
        """ä¼˜åŒ–é˜ˆå€¼é€‰æ‹©"""
        thresholds = np.linspace(evidence_data.min(), evidence_data.max(), 100)
        best_threshold = None
        best_contrast = -np.inf
        
        for threshold in thresholds:
            binary_layer = self.create_binary_layer(evidence_data, threshold)
            weights = self.calculate_weights(binary_layer, target_data)
            
            if weights['contrast'] > best_contrast:
                best_contrast = weights['contrast']
                best_threshold = threshold
        
        return best_threshold, best_contrast
```

#### è¿ç»­è¯æ®å›¾å±‚

```python
class ContinuousEvidenceLayer:
    """è¿ç»­è¯æ®å›¾å±‚æ„å»ºå™¨"""
    
    def fuzzy_membership(self, data, membership_type='linear'):
        """æ¨¡ç³Šéš¶å±åº¦è½¬æ¢"""
        if membership_type == 'linear':
            return self._linear_fuzzy(data)
        elif membership_type == 'sigmoid':
            return self._sigmoid_fuzzy(data)
        elif membership_type == 'gaussian':
            return self._gaussian_fuzzy(data)
        else:
            raise ValueError(f"æœªçŸ¥éš¶å±åº¦ç±»å‹: {membership_type}")
    
    def _linear_fuzzy(self, data):
        """çº¿æ€§æ¨¡ç³Šéš¶å±åº¦"""
        min_val = data.min()
        max_val = data.max()
        
        if max_val == min_val:
            return np.ones_like(data)
        
        return (data - min_val) / (max_val - min_val)
    
    def _sigmoid_fuzzy(self, data, k=1, x0=0):
        """Så‹æ¨¡ç³Šéš¶å±åº¦"""
        return 1 / (1 + np.exp(-k * (data - x0)))
```

### é¢„æµ‹æ¨¡å‹é›†æˆ

#### æ¨¡å‹é›†æˆæ–¹æ³•

```python
class ModelEnsemble:
    """æ¨¡å‹é›†æˆå™¨"""
    
    def __init__(self, models, weights=None):
        self.models = models
        self.weights = weights or [1.0 / len(models)] * len(models)
    
    def predict(self, data):
        """é›†æˆé¢„æµ‹"""
        predictions = []
        
        for model in self.models:
            pred = model.predict(data)
            predictions.append(pred)
        
        # åŠ æƒå¹³å‡
        ensemble_pred = np.average(predictions, axis=0, weights=self.weights)
        
        return ensemble_pred
    
    def stacking_ensemble(self, train_data, train_target, test_data):
        """å †å é›†æˆ"""
        from sklearn.model_selection import KFold
        from sklearn.linear_model import LinearRegression
        
        # ç¬¬ä¸€å±‚é¢„æµ‹
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        meta_features = np.zeros((len(train_data), len(self.models)))
        
        for i, model in enumerate(self.models):
            fold_predictions = np.zeros(len(train_data))
            
            for train_idx, val_idx in kf.split(train_data):
                model.fit(train_data[train_idx], train_target[train_idx])
                fold_predictions[val_idx] = model.predict(train_data[val_idx])
            
            meta_features[:, i] = fold_predictions
        
        # è®­ç»ƒå…ƒæ¨¡å‹
        meta_model = LinearRegression()
        meta_model.fit(meta_features, train_target)
        
        # ç¬¬äºŒå±‚é¢„æµ‹
        test_meta_features = np.zeros((len(test_data), len(self.models)))
        for i, model in enumerate(self.models):
            model.fit(train_data, train_target)
            test_meta_features[:, i] = model.predict(test_data)
        
        final_prediction = meta_model.predict(test_meta_features)
        
        return final_prediction
```

## ğŸ“ˆ æ¨¡å‹éªŒè¯ç†è®º

### äº¤å‰éªŒè¯

#### ç©ºé—´äº¤å‰éªŒè¯

```python
class SpatialCrossValidation:
    """ç©ºé—´äº¤å‰éªŒè¯"""
    
    def __init__(self, n_splits=5, spatial_cv=True):
        self.n_splits = n_splits
        self.spatial_cv = spatial_cv
    
    def spatial_split(self, coordinates, target):
        """ç©ºé—´åˆ†å‰²"""
        if self.spatial_cv:
            return self._spatial_block_split(coordinates, target)
        else:
            return self._random_split(coordinates, target)
    
    def _spatial_block_split(self, coordinates, target):
        """ç©ºé—´å—åˆ†å‰²"""
        from sklearn.cluster import KMeans
        
        # ä½¿ç”¨K-meansèšç±»åˆ†å‰²ç©ºé—´
        kmeans = KMeans(n_splits=self.n_splits, random_state=42)
        spatial_clusters = kmeans.fit_predict(coordinates)
        
        splits = []
        for i in range(self.n_splits):
            train_mask = spatial_clusters != i
            test_mask = spatial_clusters == i
            
            splits.append((train_mask, test_mask))
        
        return splits
    
    def evaluate_model(self, model, data, target, coordinates):
        """è¯„ä¼°æ¨¡å‹"""
        splits = self.spatial_split(coordinates, target)
        
        scores = []
        for train_mask, test_mask in splits:
            # è®­ç»ƒæ¨¡å‹
            model.fit(data[train_mask], target[train_mask])
            
            # é¢„æµ‹
            predictions = model.predict(data[test_mask])
            
            # è®¡ç®—è¯„åˆ†
            score = self._calculate_score(target[test_mask], predictions)
            scores.append(score)
        
        return np.mean(scores), np.std(scores)
```

### æ€§èƒ½æŒ‡æ ‡

#### åˆ†ç±»æŒ‡æ ‡

```python
class ClassificationMetrics:
    """åˆ†ç±»æ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self):
        self.metrics = {}
    
    def calculate_all_metrics(self, y_true, y_pred, y_prob=None):
        """è®¡ç®—æ‰€æœ‰åˆ†ç±»æŒ‡æ ‡"""
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score,
            roc_auc_score, confusion_matrix, classification_report
        )
        
        self.metrics['accuracy'] = accuracy_score(y_true, y_pred)
        self.metrics['precision'] = precision_score(y_true, y_pred)
        self.metrics['recall'] = recall_score(y_true, y_pred)
        self.metrics['f1'] = f1_score(y_true, y_pred)
        
        if y_prob is not None:
            self.metrics['auc'] = roc_auc_score(y_true, y_prob)
        
        self.metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)
        self.metrics['classification_report'] = classification_report(y_true, y_pred)
        
        return self.metrics
    
    def calculate_success_rate(self, predictions, target_areas, total_area):
        """è®¡ç®—æˆåŠŸç‡æ›²çº¿"""
        # æŒ‰é¢„æµ‹å€¼æ’åº
        sorted_indices = np.argsort(predictions)[::-1]
        
        success_rates = []
        area_percentages = []
        
        for i in range(1, len(sorted_indices) + 1):
            top_indices = sorted_indices[:i]
            top_area = i / len(predictions) * 100
            
            # è®¡ç®—åŒ…å«çš„ç›®æ ‡åŒºåŸŸæ¯”ä¾‹
            target_in_top = np.sum(target_areas[top_indices]) / np.sum(target_areas) * 100
            
            success_rates.append(target_in_top)
            area_percentages.append(top_area)
        
        return area_percentages, success_rates
```

## ğŸ“š ç†è®ºåº”ç”¨

### å¡æ—å‹é‡‘çŸ¿åº”ç”¨

#### åœ°çƒåŒ–å­¦ç‰¹å¾

```python
class CarlinTypeGoldAnalysis:
    """å¡æ—å‹é‡‘çŸ¿åœ°çƒåŒ–å­¦åˆ†æ"""
    
    def __init__(self):
        self.pathfinder_elements = ['Au', 'As', 'Sb', 'Hg', 'Tl', 'W']
        self.major_elements = ['Si', 'Al', 'Fe', 'Ca', 'Mg', 'Na', 'K']
    
    def identify_pathfinder_anomalies(self, geochemical_data):
        """è¯†åˆ«è·¯å¾„å…ƒç´ å¼‚å¸¸"""
        anomalies = {}
        
        for element in self.pathfinder_elements:
            if element in geochemical_data.columns:
                # ä½¿ç”¨åˆ†å½¢æ–¹æ³•æ£€æµ‹å¼‚å¸¸
                detector = FractalAnomalyDetector()
                threshold = detector.calculate_fractal_threshold(
                    geochemical_data[element]
                )
                
                anomalies[element] = {
                    'threshold': threshold,
                    'anomaly_points': geochemical_data[geochemical_data[element] > threshold]
                }
        
        return anomalies
    
    def calculate_gold_association_index(self, geochemical_data):
        """è®¡ç®—é‡‘å…³è”æŒ‡æ•°"""
        if 'Au' not in geochemical_data.columns:
            return None
        
        gold_correlations = {}
        for element in self.pathfinder_elements:
            if element != 'Au' and element in geochemical_data.columns:
                correlation = np.corrcoef(
                    geochemical_data['Au'], 
                    geochemical_data[element]
                )[0, 1]
                gold_correlations[element] = correlation
        
        return gold_correlations
```

### æ–‘å²©å‹é“œçŸ¿åº”ç”¨

#### åœ°çƒåŒ–å­¦ç‰¹å¾

```python
class PorphyryCopperAnalysis:
    """æ–‘å²©å‹é“œçŸ¿åœ°çƒåŒ–å­¦åˆ†æ"""
    
    def __init__(self):
        self.pathfinder_elements = ['Cu', 'Mo', 'Au', 'Ag', 'Re']
        self.alteration_elements = ['K', 'Na', 'Ca', 'Mg', 'Fe', 'Al']
    
    def identify_alteration_zones(self, geochemical_data):
        """è¯†åˆ«èš€å˜å¸¦"""
        alteration_indices = {}
        
        # é’¾åŒ–æŒ‡æ•°
        if 'K' in geochemical_data.columns and 'Na' in geochemical_data.columns:
            k_na_ratio = geochemical_data['K'] / geochemical_data['Na']
            alteration_indices['potassic'] = k_na_ratio
        
        # é’ç£å²©åŒ–æŒ‡æ•°
        if 'Ca' in geochemical_data.columns and 'Na' in geochemical_data.columns:
            ca_na_ratio = geochemical_data['Ca'] / geochemical_data['Na']
            alteration_indices['propylitic'] = ca_na_ratio
        
        # æ³¥åŒ–æŒ‡æ•°
        if 'Al' in geochemical_data.columns and 'K' in geochemical_data.columns:
            al_k_ratio = geochemical_data['Al'] / geochemical_data['K']
            alteration_indices['argillic'] = al_k_ratio
        
        return alteration_indices
    
    def calculate_copper_potential(self, geochemical_data):
        """è®¡ç®—é“œçŸ¿æ½œåŠ›"""
        if 'Cu' not in geochemical_data.columns:
            return None
        
        # å¤šå…ƒç´ ç»¼åˆæŒ‡æ•°
        elements = ['Cu', 'Mo', 'Au']
        available_elements = [e for e in elements if e in geochemical_data.columns]
        
        if len(available_elements) < 2:
            return None
        
        # æ ‡å‡†åŒ–æ•°æ®
        normalized_data = geochemical_data[available_elements].apply(
            lambda x: (x - x.min()) / (x.max() - x.min())
        )
        
        # è®¡ç®—ç»¼åˆæŒ‡æ•°
        copper_potential = normalized_data.mean(axis=1)
        
        return copper_potential
```

## ğŸ”® ç†è®ºå‘å±•

### æ–°å…´ç†è®ºæ–¹å‘

#### æœºå™¨å­¦ä¹ é›†æˆ

```python
class MLIntegratedGeochemistry:
    """æœºå™¨å­¦ä¹ é›†æˆçš„åœ°çƒåŒ–å­¦åˆ†æ"""
    
    def __init__(self):
        self.traditional_methods = {
            'statistical': StatisticalAnomalyDetector(),
            'fractal': FractalAnomalyDetector(),
            'multivariate': MultivariateAnomalyDetector()
        }
        self.ml_methods = {
            'random_forest': None,
            'svm': None,
            'neural_network': None
        }
    
    def hybrid_anomaly_detection(self, data):
        """æ··åˆå¼‚å¸¸æ£€æµ‹"""
        # ä¼ ç»Ÿæ–¹æ³•ç»“æœ
        traditional_results = {}
        for method_name, method in self.traditional_methods.items():
            traditional_results[method_name] = method.detect_outliers(data)
        
        # æœºå™¨å­¦ä¹ æ–¹æ³•ç»“æœ
        ml_results = {}
        for method_name, model in self.ml_methods.items():
            if model is not None:
                ml_results[method_name] = model.predict(data)
        
        # é›†æˆç»“æœ
        ensemble_results = self._ensemble_results(
            traditional_results, ml_results
        )
        
        return ensemble_results
```

#### æ·±åº¦å­¦ä¹ åº”ç”¨

```python
class DeepLearningGeochemistry:
    """æ·±åº¦å­¦ä¹ åœ°çƒåŒ–å­¦åˆ†æ"""
    
    def __init__(self):
        self.autoencoder = None
        self.cnn_model = None
        self.lstm_model = None
    
    def build_autoencoder(self, input_dim):
        """æ„å»ºè‡ªç¼–ç å™¨"""
        from tensorflow.keras.models import Model
        from tensorflow.keras.layers import Input, Dense
        
        # ç¼–ç å™¨
        input_layer = Input(shape=(input_dim,))
        encoded = Dense(64, activation='relu')(input_layer)
        encoded = Dense(32, activation='relu')(encoded)
        encoded = Dense(16, activation='relu')(encoded)
        
        # è§£ç å™¨
        decoded = Dense(32, activation='relu')(encoded)
        decoded = Dense(64, activation='relu')(decoded)
        decoded = Dense(input_dim, activation='linear')(decoded)
        
        # è‡ªç¼–ç å™¨æ¨¡å‹
        autoencoder = Model(input_layer, decoded)
        autoencoder.compile(optimizer='adam', loss='mse')
        
        self.autoencoder = autoencoder
        return autoencoder
    
    def detect_anomalies_with_autoencoder(self, data):
        """ä½¿ç”¨è‡ªç¼–ç å™¨æ£€æµ‹å¼‚å¸¸"""
        if self.autoencoder is None:
            self.build_autoencoder(data.shape[1])
        
        # è®­ç»ƒè‡ªç¼–ç å™¨
        self.autoencoder.fit(data, data, epochs=100, batch_size=32, verbose=0)
        
        # é‡æ„æ•°æ®
        reconstructed = self.autoencoder.predict(data)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        reconstruction_errors = np.mean((data - reconstructed) ** 2, axis=1)
        
        # åŸºäºè¯¯å·®æ£€æµ‹å¼‚å¸¸
        threshold = np.percentile(reconstruction_errors, 95)
        anomalies = reconstruction_errors > threshold
        
        return anomalies, reconstruction_errors
```

## ğŸ“š æ€»ç»“

Carranzaç†è®ºä¸ºGold-Seekerå¹³å°æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€ï¼Œå…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

1. **ç³»ç»ŸåŒ–çš„æ–¹æ³•è®º**: ä»æ•°æ®é¢„å¤„ç†åˆ°æœ€ç»ˆé¢„æµ‹çš„å®Œæ•´æµç¨‹
2. **å¤šå­¦ç§‘èåˆ**: ç»“åˆåœ°çƒåŒ–å­¦ã€ç»Ÿè®¡å­¦ã€GISå’Œæœºå™¨å­¦ä¹ 
3. **å®ç”¨æ€§å¯¼å‘**: ç†è®ºæ–¹æ³•å¯ç›´æ¥åº”ç”¨äºå®é™…å‹˜æ¢å·¥ä½œ
4. **å¯æ‰©å±•æ€§**: ç†è®ºæ¡†æ¶å¯ä»¥å®¹çº³æ–°çš„æ–¹æ³•å’ŒæŠ€æœ¯

Gold-Seekerå¹³å°åœ¨Carranzaç†è®ºåŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å‘å±•äº†ï¼š

1. **å¤šæ™ºèƒ½ä½“æ¶æ„**: å®ç°äº†ä¸“ä¸šåŒ–çš„æ™ºèƒ½åˆ†å·¥
2. **è‡ªåŠ¨åŒ–æµç¨‹**: å‡å°‘äº†äººå·¥å¹²é¢„ï¼Œæé«˜äº†æ•ˆç‡
3. **é›†æˆåŒ–åˆ†æ**: æ”¯æŒå¤šç§æ–¹æ³•çš„ç»¼åˆåº”ç”¨
4. **æ™ºèƒ½åŒ–å†³ç­–**: åŸºäºAIçš„æ™ºèƒ½åˆ†æå’Œå»ºè®®

è¿™ç§ç†è®ºä¸å®è·µçš„ç»“åˆï¼Œä½¿å¾—Gold-Seekeræˆä¸ºç°ä»£åœ°çƒåŒ–å­¦å‹˜æ¢çš„å¼ºå¤§å·¥å…·ã€‚