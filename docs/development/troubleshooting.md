# Gold-Seeker æ•…éšœæ’é™¤æŒ‡å—

æœ¬æŒ‡å—æä¾›Gold-Seekeråœ°çƒåŒ–å­¦æ‰¾çŸ¿é¢„æµ‹æ™ºèƒ½å¹³å°çš„å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆå’Œæ•…éšœæ’é™¤æ–¹æ³•ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿè¯Šæ–­](#å¿«é€Ÿè¯Šæ–­)
- [å®‰è£…é—®é¢˜](#å®‰è£…é—®é¢˜)
- [é…ç½®é—®é¢˜](#é…ç½®é—®é¢˜)
- [æ•°æ®é—®é¢˜](#æ•°æ®é—®é¢˜)
- [åˆ†æé—®é¢˜](#åˆ†æé—®é¢˜)
- [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
- [ç½‘ç»œé—®é¢˜](#ç½‘ç»œé—®é¢˜)
- [ç³»ç»Ÿé—®é¢˜](#ç³»ç»Ÿé—®é¢˜)
- [è°ƒè¯•å·¥å…·](#è°ƒè¯•å·¥å…·)

## ğŸ” å¿«é€Ÿè¯Šæ–­

### 1. ç³»ç»Ÿå¥åº·æ£€æŸ¥

```bash
# è¿è¡Œç³»ç»Ÿè¯Šæ–­
gold-seeker doctor

# æ£€æŸ¥é…ç½®
gold-seeker validate --config config/production.yaml

# æµ‹è¯•æ•°æ®åº“è¿æ¥
gold-seeker test --database

# æµ‹è¯•æ‰€æœ‰ç»„ä»¶
gold-seeker test --all
```

### 2. æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f /var/log/gold-seeker/gold_seeker.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep -i error /var/log/gold-seeker/gold_seeker.log

# æŸ¥çœ‹æœ€è¿‘çš„è­¦å‘Š
grep -i warning /var/log/gold-seeker/gold_seeker.log | tail -20

# åˆ†ææ—¥å¿—æ¨¡å¼
awk '{print $1}' /var/log/gold-seeker/gold_seeker.log | sort | uniq -c | sort -nr
```

### 3. æ€§èƒ½ç›‘æ§

```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
top -p $(pgrep -f gold-seeker)
htop -p $(pgrep -f gold-seeker)

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
ps aux | grep gold-seeker | awk '{sum+=$6} END {print "Memory:", sum/1024, "MB"}'

# æ£€æŸ¥ç£ç›˜ä½¿ç”¨
df -h /var/lib/gold-seeker
du -sh /var/lib/gold-seeker/*
```

## ğŸ› ï¸ å®‰è£…é—®é¢˜

### é—®é¢˜1: ä¾èµ–å®‰è£…å¤±è´¥

#### ç—‡çŠ¶
```
ERROR: Could not install packages due to an EnvironmentError
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. å‡çº§pip
python -m pip install --upgrade pip

# 2. æ¸…ç†ç¼“å­˜
pip cache purge

# 3. ä½¿ç”¨å›½å†…é•œåƒ
pip install -e ".[all]" -i https://pypi.tuna.tsinghua.edu.cn/simple/

# 4. åˆ†æ­¥å®‰è£…æ ¸å¿ƒä¾èµ–
pip install numpy pandas scipy
pip install geopandas rasterio
pip install scikit-learn matplotlib
pip install -e ".[dev]"
```

#### GDALå®‰è£…é—®é¢˜

```bash
# Ubuntu/Debian
sudo apt-get install -y gdal-bin libgdal-dev
export CPLUS_INCLUDE_PATH=/usr/include/gdal
export C_INCLUDE_PATH=/usr/include/gdal
pip install GDAL==$(gdal-config --version)

# CentOS/RHEL
sudo yum install -y gdal gdal-devel
export GDAL_CONFIG=/usr/bin/gdal-config
pip install GDAL

# macOS
brew install gdal
pip install GDAL
```

### é—®é¢˜2: æƒé™é”™è¯¯

#### ç—‡çŠ¶
```
PermissionError: [Errno 13] Permission denied
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. ä½¿ç”¨ç”¨æˆ·å®‰è£…
pip install --user -e ".[all]"

# 2. ä¿®å¤æƒé™
sudo chown -R $USER:$USER ~/.local
sudo chown -R $USER:$USER /usr/local/lib/python3.10/site-packages

# 3. ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
python -m venv gold-seeker-env
source gold-seeker-env/bin/activate
pip install -e ".[all]"
```

### é—®é¢˜3: ç‰ˆæœ¬å†²çª

#### ç—‡çŠ¶
```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. åˆ›å»ºå¹²å‡€ç¯å¢ƒ
python -m venv fresh-env
source fresh-env/bin/activate

# 2. ä½¿ç”¨pip-tools
pip install pip-tools
pip-compile requirements.in
pip-sync requirements.txt

# 3. å¼ºåˆ¶é‡æ–°å®‰è£…
pip install --force-reinstall --no-cache-dir -e ".[all]"
```

## âš™ï¸ é…ç½®é—®é¢˜

### é—®é¢˜1: é…ç½®æ–‡ä»¶é”™è¯¯

#### ç—‡çŠ¶
```
ConfigError: Invalid configuration file
```

#### è¯Šæ–­å·¥å…·

```python
# config_validator.py
import yaml
from pathlib import Path

def validate_config(config_path):
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_sections = ['project', 'data', 'analysis', 'logging']
        for section in required_sections:
            if section not in config:
                print(f"âŒ ç¼ºå°‘å¿…éœ€éƒ¨åˆ†: {section}")
                return False
        
        # æ£€æŸ¥è·¯å¾„
        data_dir = Path(config['data']['data_dir'])
        if not data_dir.exists():
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return False
        
        print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
        return True
        
    except yaml.YAMLError as e:
        print(f"âŒ YAMLè¯­æ³•é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    validate_config("config/default_config.yaml")
```

#### å¸¸è§é…ç½®é”™è¯¯

```yaml
# âŒ é”™è¯¯: ç¼ºå°‘å¿…éœ€å­—æ®µ
project:
  name: "æµ‹è¯•"  # ç¼ºå°‘environmentå­—æ®µ

# âœ… æ­£ç¡®
project:
  name: "æµ‹è¯•"
  environment: "development"

# âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨
data:
  data_dir: "/nonexistent/path"

# âœ… æ­£ç¡®
data:
  data_dir: "./data"
```

### é—®é¢˜2: ç¯å¢ƒå˜é‡æœªè®¾ç½®

#### ç—‡çŠ¶
```
KeyError: 'SECRET_KEY'
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. åˆ›å»º.envæ–‡ä»¶
cat > .env << EOF
SECRET_KEY=your-secret-key-here
DB_PASSWORD=your-database-password
API_KEY=your-api-key
EOF

# 2. åŠ è½½ç¯å¢ƒå˜é‡
export $(cat .env | xargs)

# 3. åœ¨Pythonä¸­åŠ è½½
from dotenv import load_dotenv
load_dotenv()
```

### é—®é¢˜3: æ•°æ®åº“è¿æ¥å¤±è´¥

#### ç—‡çŠ¶
```
ConnectionError: Could not connect to database
```

#### è¯Šæ–­è„šæœ¬

```python
# db_test.py
import psycopg2
import redis

def test_postgres(config):
    """æµ‹è¯•PostgreSQLè¿æ¥"""
    try:
        conn = psycopg2.connect(
            host=config['database']['host'],
            port=config['database']['port'],
            database=config['database']['name'],
            user=config['database']['user'],
            password=config['database']['password']
        )
        
        with conn.cursor() as cur:
            cur.execute("SELECT version()")
            version = cur.fetchone()[0]
            print(f"âœ… PostgreSQLè¿æ¥æˆåŠŸ: {version}")
        
        conn.close()
        return True
        
    except Exception as e:
        print(f"âŒ PostgreSQLè¿æ¥å¤±è´¥: {e}")
        return False

def test_redis(config):
    """æµ‹è¯•Redisè¿æ¥"""
    try:
        r = redis.Redis(
            host=config['redis']['host'],
            port=config['redis']['port'],
            db=config['redis']['db']
        )
        
        r.ping()
        print("âœ… Redisè¿æ¥æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
        return False
```

## ğŸ“Š æ•°æ®é—®é¢˜

### é—®é¢˜1: æ•°æ®æ ¼å¼é”™è¯¯

#### ç—‡çŠ¶
```
ValueError: Could not convert string to float
```

#### æ•°æ®è¯Šæ–­å·¥å…·

```python
# data_diagnostic.py
import pandas as pd
import numpy as np
from pathlib import Path

def diagnose_data(file_path):
    """è¯Šæ–­æ•°æ®æ–‡ä»¶"""
    try:
        # å°è¯•è¯»å–æ•°æ®
        if file_path.suffix.lower() == '.csv':
            df = pd.read_csv(file_path)
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
            return False
        
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_values = df.isnull().sum()
        if missing_values.any():
            print("âš ï¸ ç¼ºå¤±å€¼ç»Ÿè®¡:")
            for col, count in missing_values[missing_values > 0].items():
                print(f"  {col}: {count} ({count/len(df)*100:.1f}%)")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        print("\nğŸ“ æ•°æ®ç±»å‹:")
        for col, dtype in df.dtypes.items():
            print(f"  {col}: {dtype}")
        
        # æ£€æŸ¥æ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\nğŸ”¢ æ•°å€¼åˆ—ç»Ÿè®¡:")
            print(df[numeric_cols].describe())
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
            if len(outliers) > 0:
                print(f"âš ï¸ {col} å‘ç° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
        
        print("âœ… æ•°æ®è¯Šæ–­å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®è¯Šæ–­å¤±è´¥: {e}")
        return False
```

#### æ•°æ®ä¿®å¤å·¥å…·

```python
# data_fixer.py
import pandas as pd
import numpy as np

def fix_data_issues(df):
    """ä¿®å¤å¸¸è§æ•°æ®é—®é¢˜"""
    # 1. å¤„ç†ç¼ºå¤±å€¼
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
    
    # 2. å¤„ç†å¼‚å¸¸å€¼
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    
    # 3. æ ‡å‡†åŒ–åˆ—å
    df.columns = df.columns.str.lower().str.replace(' ', '_')
    
    return df
```

### é—®é¢˜2: åæ ‡ç³»ç»Ÿé”™è¯¯

#### ç—‡çŠ¶
```
CRSError: Invalid coordinate reference system
```

#### è§£å†³æ–¹æ¡ˆ

```python
# crs_fixer.py
import geopandas as gpd
from pyproj import CRS

def fix_crs(gdf, target_crs='EPSG:4326'):
    """ä¿®å¤åæ ‡ç³»ç»Ÿ"""
    try:
        # æ£€æŸ¥å½“å‰CRS
        if gdf.crs is None:
            print("âš ï¸ æ•°æ®æ²¡æœ‰CRSä¿¡æ¯ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹...")
            # å‡è®¾æ•°æ®æ˜¯WGS84
            gdf.crs = 'EPSG:4326'
        
        # è½¬æ¢åˆ°ç›®æ ‡CRS
        if gdf.crs != target_crs:
            print(f"ğŸ”„ è½¬æ¢CRS: {gdf.crs} -> {target_crs}")
            gdf = gdf.to_crs(target_crs)
        
        # éªŒè¯å‡ ä½•æœ‰æ•ˆæ€§
        invalid_geoms = gdf[~gdf.geometry.is_valid]
        if len(invalid_geoms) > 0:
            print(f"âš ï¸ å‘ç° {len(invalid_geoms)} ä¸ªæ— æ•ˆå‡ ä½•ï¼Œå°è¯•ä¿®å¤...")
            gdf.geometry = gdf.geometry.buffer(0)
        
        print("âœ… CRSä¿®å¤å®Œæˆ")
        return gdf
        
    except Exception as e:
        print(f"âŒ CRSä¿®å¤å¤±è´¥: {e}")
        return None
```

### é—®é¢˜3: æ•°æ®é‡è¿‡å¤§

#### ç—‡çŠ¶
```
MemoryError: Unable to allocate array
```

#### è§£å†³æ–¹æ¡ˆ

```python
# data_chunker.py
import pandas as pd
import numpy as np

def process_large_data(file_path, chunk_size=10000):
    """åˆ†å—å¤„ç†å¤§æ•°æ®"""
    chunks = []
    
    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
        print(f"å¤„ç†å— {i+1}: {len(chunk)} è¡Œ")
        
        # å¤„ç†æ•°æ®
        processed_chunk = process_chunk(chunk)
        chunks.append(processed_chunk)
    
    # åˆå¹¶ç»“æœ
    result = pd.concat(chunks, ignore_index=True)
    return result

def process_chunk(chunk):
    """å¤„ç†å•ä¸ªæ•°æ®å—"""
    # å®ç°å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘
    return chunk
```

## ğŸ”¬ åˆ†æé—®é¢˜

### é—®é¢˜1: æ¨¡å‹è®­ç»ƒå¤±è´¥

#### ç—‡çŠ¶
```
ValueError: Input contains NaN, infinity or a value too large
```

#### è¯Šæ–­å·¥å…·

```python
# model_diagnostic.py
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

def diagnose_training_data(X, y):
    """è¯Šæ–­è®­ç»ƒæ•°æ®"""
    print("ğŸ” è®­ç»ƒæ•°æ®è¯Šæ–­")
    
    # æ£€æŸ¥å½¢çŠ¶
    print(f"Xå½¢çŠ¶: {X.shape}")
    print(f"yå½¢çŠ¶: {y.shape}")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    if np.isnan(X).any():
        print("âŒ XåŒ…å«NaNå€¼")
        return False
    
    if np.isnan(y).any():
        print("âŒ yåŒ…å«NaNå€¼")
        return False
    
    # æ£€æŸ¥æ— ç©·å€¼
    if np.isinf(X).any():
        print("âŒ XåŒ…å«æ— ç©·å€¼")
        return False
    
    if np.isinf(y).any():
        print("âŒ yåŒ…å«æ— ç©·å€¼")
        return False
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    if not np.issubdtype(X.dtype, np.number):
        print("âŒ Xä¸æ˜¯æ•°å€¼ç±»å‹")
        return False
    
    if not np.issubdtype(y.dtype, np.number):
        print("âŒ yä¸æ˜¯æ•°å€¼ç±»å‹")
        return False
    
    # æ£€æŸ¥æ•°æ®èŒƒå›´
    print(f"XèŒƒå›´: [{X.min():.3f}, {X.max():.3f}]")
    print(f"yèŒƒå›´: [{y.min():.3f}, {y.max():.3f}]")
    
    # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    if len(np.unique(y)) < 2:
        print("âŒ yåªæœ‰ä¸€ä¸ªç±»åˆ«")
        return False
    
    print("âœ… è®­ç»ƒæ•°æ®æ­£å¸¸")
    return True

def preprocess_data(X, y):
    """é¢„å¤„ç†æ•°æ®"""
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # å¤„ç†å¼‚å¸¸å€¼
    X_scaled = np.clip(X_scaled, -3, 3)
    
    return X_scaled, y, scaler
```

### é—®é¢˜2: åˆ†æ•°è®¡ç®—é”™è¯¯

#### ç—‡çŠ¶
```
ZeroDivisionError: Division by zero in weights calculation
```

#### è§£å†³æ–¹æ¡ˆ

```python
# weights_fixer.py
import numpy as np
import pandas as pd

def safe_weights_calculation(evidence, target):
    """å®‰å…¨çš„æƒé‡è®¡ç®—"""
    try:
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡
        total_area = len(evidence)
        target_area = np.sum(target)
        non_target_area = total_area - target_area
        
        # é¿å…é™¤é›¶é”™è¯¯
        if target_area == 0:
            print("âš ï¸ ç›®æ ‡åŒºåŸŸä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æƒé‡")
            return None, None, None
        
        if non_target_area == 0:
            print("âš ï¸ éç›®æ ‡åŒºåŸŸä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æƒé‡")
            return None, None, None
        
        # è®¡ç®—æƒé‡
        w_plus = np.log((target_area / total_area) / (np.sum(evidence[target == 1]) / np.sum(evidence)))
        w_minus = np.log((non_target_area / total_area) / (np.sum(evidence[target == 0]) / np.sum(evidence)))
        contrast = w_plus - w_minus
        
        return w_plus, w_minus, contrast
        
    except Exception as e:
        print(f"âŒ æƒé‡è®¡ç®—å¤±è´¥: {e}")
        return None, None, None
```

### é—®é¢˜3: åˆ†å½¢åˆ†æå¤±è´¥

#### ç—‡çŠ¶
```
LinAlgError: SVD did not converge
```

#### è§£å†³æ–¹æ¡ˆ

```python
# fractal_fixer.py
import numpy as np
from scipy.optimize import curve_fit
from scipy.stats import linregress

def robust_fractal_analysis(x, y):
    """é²æ£’çš„åˆ†å½¢åˆ†æ"""
    try:
        # ç§»é™¤æ— æ•ˆå€¼
        valid_idx = ~np.isnan(x) & ~np.isnan(y) & (x > 0) & (y > 0)
        x_clean = x[valid_idx]
        y_clean = y[valid_idx]
        
        if len(x_clean) < 3:
            print("âŒ æœ‰æ•ˆæ•°æ®ç‚¹å¤ªå°‘")
            return None, None, None
        
        # å¯¹æ•°å˜æ¢
        log_x = np.log10(x_clean)
        log_y = np.log10(y_clean)
        
        # çº¿æ€§å›å½’
        slope, intercept, r_value, p_value, std_err = linregress(log_x, log_y)
        
        # è®¡ç®—æ‹Ÿåˆä¼˜åº¦
        y_pred = slope * log_x + intercept
        ss_res = np.sum((log_y - y_pred) ** 2)
        ss_tot = np.sum((log_y - np.mean(log_y)) ** 2)
        r_squared = 1 - (ss_res / ss_tot)
        
        return slope, intercept, r_squared
        
    except Exception as e:
        print(f"âŒ åˆ†å½¢åˆ†æå¤±è´¥: {e}")
        return None, None, None
```

## âš¡ æ€§èƒ½é—®é¢˜

### é—®é¢˜1: å†…å­˜ä½¿ç”¨è¿‡é«˜

#### ç—‡çŠ¶
```
MemoryError: Unable to allocate array
```

#### è§£å†³æ–¹æ¡ˆ

```python
# memory_optimizer.py
import gc
import psutil
import numpy as np
import pandas as pd

def monitor_memory():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    memory_info = process.memory_info()
    memory_mb = memory_info.rss / 1024 / 1024
    
    print(f"ğŸ§  å†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
    return memory_mb

def optimize_memory(df):
    """ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨"""
    start_memory = monitor_memory()
    
    # æ•°å€¼åˆ—ä¼˜åŒ–
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    
    # å­—ç¬¦ä¸²åˆ—ä¼˜åŒ–
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) < 0.5:  # ä½åŸºæ•°
            df[col] = df[col].astype('category')
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    
    end_memory = monitor_memory()
    print(f"ğŸ’¾ å†…å­˜èŠ‚çœ: {start_memory - end_memory:.1f} MB")
    
    return df

def process_with_memory_limit(func, data, max_memory_mb=1000):
    """åœ¨å†…å­˜é™åˆ¶ä¸‹å¤„ç†æ•°æ®"""
    def process_chunk(chunk):
        return func(chunk)
    
    # å¦‚æœæ•°æ®å¤ªå¤§ï¼Œåˆ†å—å¤„ç†
    if isinstance(data, pd.DataFrame) and len(data) > 100000:
        chunk_size = 10000
        results = []
        
        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i+chunk_size]
            result = process_chunk(chunk)
            results.append(result)
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            if monitor_memory() > max_memory_mb:
                gc.collect()
        
        return pd.concat(results, ignore_index=True)
    else:
        return process_chunk(data)
```

### é—®é¢˜2: å¤„ç†é€Ÿåº¦æ…¢

#### ç—‡çŠ¶
```
å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œç”¨æˆ·ç­‰å¾…è¶…æ—¶
```

#### è§£å†³æ–¹æ¡ˆ

```python
# performance_optimizer.py
import multiprocessing as mp
import numpy as np
import pandas as pd
from functools import partial
from concurrent.futures import ProcessPoolExecutor

def parallel_apply(df, func, n_workers=None):
    """å¹¶è¡Œåº”ç”¨å‡½æ•°"""
    if n_workers is None:
        n_workers = mp.cpu_count() - 1
    
    # åˆ†å‰²æ•°æ®
    chunks = np.array_split(df, n_workers)
    
    # å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        results = list(executor.map(func, chunks))
    
    # åˆå¹¶ç»“æœ
    return pd.concat(results, ignore_index=True)

def vectorized_operation(df, columns):
    """å‘é‡åŒ–æ“ä½œ"""
    # ä½¿ç”¨numpyå‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
    data = df[columns].values
    
    # ç¤ºä¾‹ï¼šæ ‡å‡†åŒ–
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    normalized = (data - mean) / std
    
    result_df = df.copy()
    result_df[columns] = normalized
    
    return result_df

def cache_result(func):
    """ç¼“å­˜è£…é¥°å™¨"""
    cache = {}
    
    def wrapper(*args, **kwargs):
        key = str(args) + str(sorted(kwargs.items()))
        
        if key in cache:
            return cache[key]
        
        result = func(*args, **kwargs)
        cache[key] = result
        
        return result
    
    return wrapper
```

### é—®é¢˜3: å¹¶å‘é—®é¢˜

#### ç—‡çŠ¶
```
DeadlockError: deadlock detected
```

#### è§£å†³æ–¹æ¡ˆ

```python
# concurrency_fixer.py
import threading
import queue
import time
from contextlib import contextmanager

class ThreadSafeCounter:
    """çº¿ç¨‹å®‰å…¨è®¡æ•°å™¨"""
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()
    
    def increment(self):
        with self._lock:
            self._value += 1
            return self._value
    
    def get(self):
        with self._lock:
            return self._value

@contextmanager
def database_lock(db_connection, timeout=30):
    """æ•°æ®åº“é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        # è·å–é”
        db_connection.execute("SELECT pg_advisory_lock(12345)")
        yield db_connection
    finally:
        # é‡Šæ”¾é”
        db_connection.execute("SELECT pg_advisory_unlock(12345)")

def worker_with_retry(queue, result_queue, max_retries=3):
    """å¸¦é‡è¯•çš„å·¥ä½œçº¿ç¨‹"""
    while True:
        try:
            task = queue.get(timeout=1)
            
            for attempt in range(max_retries):
                try:
                    result = process_task(task)
                    result_queue.put(result)
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        result_queue.put(('error', str(e)))
                        break
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            
            queue.task_done()
            
        except queue.Empty:
            break
```

## ğŸŒ ç½‘ç»œé—®é¢˜

### é—®é¢˜1: APIè¿æ¥å¤±è´¥

#### ç—‡çŠ¶
```
ConnectionError: Failed to establish connection
```

#### è§£å†³æ–¹æ¡ˆ

```python
# network_fixer.py
import requests
import time
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

def create_robust_session(max_retries=3, backoff_factor=0.3):
    """åˆ›å»ºé²æ£’çš„HTTPä¼šè¯"""
    session = requests.Session()
    
    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=max_retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS", "POST", "PUT", "DELETE"]
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # è®¾ç½®è¶…æ—¶
    session.timeout = (10, 30)  # è¿æ¥è¶…æ—¶ï¼Œè¯»å–è¶…æ—¶
    
    return session

def safe_api_call(session, url, method='GET', **kwargs):
    """å®‰å…¨çš„APIè°ƒç”¨"""
    try:
        response = session.request(method, url, **kwargs)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶")
        return None
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥é”™è¯¯")
        return None
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return None
```

### é—®é¢˜2: ä»£ç†é…ç½®é—®é¢˜

#### ç—‡çŠ¶
```
ProxyError: HTTPSConnectionPool failed
```

#### è§£å†³æ–¹æ¡ˆ

```python
# proxy_fixer.py
import os
import requests

def configure_proxy():
    """é…ç½®ä»£ç†"""
    # ä»ç¯å¢ƒå˜é‡è¯»å–ä»£ç†è®¾ç½®
    http_proxy = os.environ.get('HTTP_PROXY')
    https_proxy = os.environ.get('HTTPS_PROXY')
    
    if http_proxy or https_proxy:
        proxies = {
            'http': http_proxy,
            'https': https_proxy
        }
        return proxies
    else:
        return None

def test_proxy_connection():
    """æµ‹è¯•ä»£ç†è¿æ¥"""
    proxies = configure_proxy()
    
    try:
        response = requests.get(
            'https://httpbin.org/ip',
            proxies=proxies,
            timeout=10
        )
        print(f"âœ… ä»£ç†è¿æ¥æˆåŠŸ: {response.json()}")
        return True
    except Exception as e:
        print(f"âŒ ä»£ç†è¿æ¥å¤±è´¥: {e}")
        return False
```

## ğŸ”§ ç³»ç»Ÿé—®é¢˜

### é—®é¢˜1: æœåŠ¡å¯åŠ¨å¤±è´¥

#### ç—‡çŠ¶
```
SystemExit: Error starting server
```

#### è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# service_diagnostic.sh

echo "ğŸ” æœåŠ¡è¯Šæ–­å¼€å§‹..."

# æ£€æŸ¥ç«¯å£å ç”¨
echo "ğŸ“¡ æ£€æŸ¥ç«¯å£å ç”¨:"
netstat -tlnp | grep :8000
netstat -tlnp | grep :8080

# æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
echo "ğŸ”„ æ£€æŸ¥è¿›ç¨‹çŠ¶æ€:"
ps aux | grep gold-seeker

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo "ğŸ’¾ æ£€æŸ¥ç³»ç»Ÿèµ„æº:"
free -h
df -h

# æ£€æŸ¥æ—¥å¿—
echo "ğŸ“‹ æ£€æŸ¥æœ€è¿‘çš„é”™è¯¯æ—¥å¿—:"
tail -20 /var/log/gold-seeker/gold_seeker.log | grep -i error

# æ£€æŸ¥é…ç½®æ–‡ä»¶
echo "âš™ï¸ æ£€æŸ¥é…ç½®æ–‡ä»¶:"
gold-seeker validate --config /etc/gold-seeker/config.yaml

echo "ğŸ” æœåŠ¡è¯Šæ–­å®Œæˆ"
```

### é—®é¢˜2: æƒé™é—®é¢˜

#### ç—‡çŠ¶
```
PermissionError: [Errno 13] Permission denied
```

#### è§£å†³æ–¹æ¡ˆ

```bash
#!/bin/bash
# permission_fix.sh

# è®¾ç½®æ­£ç¡®çš„æ–‡ä»¶æƒé™
sudo chown -R gold-seeker:gold-seeker /var/lib/gold-seeker
sudo chmod -R 755 /var/lib/gold-seeker

# è®¾ç½®æ—¥å¿—ç›®å½•æƒé™
sudo chown -R gold-seeker:gold-seeker /var/log/gold-seeker
sudo chmod -R 755 /var/log/gold-seeker

# è®¾ç½®é…ç½®æ–‡ä»¶æƒé™
sudo chown gold-seeker:gold-seeker /etc/gold-seeker/config.yaml
sudo chmod 644 /etc/gold-seeker/config.yaml

# æ·»åŠ ç”¨æˆ·åˆ°å¿…è¦ç»„
sudo usermod -aG docker gold-seeker
sudo usermod -aG redis gold-seeker
```

## ğŸ› ï¸ è°ƒè¯•å·¥å…·

### 1. æ€§èƒ½åˆ†æå™¨

```python
# profiler.py
import cProfile
import pstats
import io
from functools import wraps

def profile_function(func):
    """å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
        pr = cProfile.Profile()
        
        # å¼€å§‹åˆ†æ
        pr.enable()
        result = func(*args, **kwargs)
        pr.disable()
        
        # è¾“å‡ºç»“æœ
        s = io.StringIO()
        ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
        ps.print_stats(10)  # æ˜¾ç¤ºå‰10ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
        
        print(f"ğŸ” {func.__name__} æ€§èƒ½åˆ†æ:")
        print(s.getvalue())
        
        return result
    
    return wrapper

# ä½¿ç”¨ç¤ºä¾‹
@profile_function
def slow_function():
    import time
    time.sleep(1)
    return "done"
```

### 2. å†…å­˜åˆ†æå™¨

```python
# memory_profiler.py
import tracemalloc
from functools import wraps

def memory_profile(func):
    """å†…å­˜ä½¿ç”¨åˆ†æè£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # å¼€å§‹å†…å­˜è·Ÿè¸ª
        tracemalloc.start()
        
        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        print(f"ğŸ§  {func.__name__} å†…å­˜ä½¿ç”¨:")
        print(f"  å½“å‰: {current / 1024 / 1024:.1f} MB")
        print(f"  å³°å€¼: {peak / 1024 / 1024:.1f} MB")
        
        return result
    
    return wrapper
```

### 3. æ—¥å¿—å¢å¼ºå™¨

```python
# logger_enhancer.py
import logging
import traceback
import functools
import time

def enhanced_logger(logger_name='gold_seeker'):
    """å¢å¼ºçš„æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(logger_name)
    
    # åˆ›å»ºè¯¦ç»†æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - '
        '%(filename)s:%(lineno)d - %(funcName)s - %(message)s'
    )
    
    # æ·»åŠ å¤„ç†å™¨
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

def log_exceptions(func):
    """å¼‚å¸¸æ—¥å¿—è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = enhanced_logger()
        
        try:
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            
            logger.info(f"âœ… {func.__name__} æˆåŠŸå®Œæˆ ({end_time - start_time:.2f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {func.__name__} å¤±è´¥: {str(e)}")
            logger.error(f"ğŸ“‹ å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
            raise
    
    return wrapper
```

### 4. ç³»ç»Ÿç›‘æ§å™¨

```python
# system_monitor.py
import psutil
import time
import threading
from datetime import datetime

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, interval=60):
        self.interval = interval
        self.running = False
        self.thread = None
    
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.daemon = True
        self.thread.start()
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.thread:
            self.thread.join()
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            self._log_system_status()
            time.sleep(self.interval)
    
    def _log_system_status(self):
        """è®°å½•ç³»ç»ŸçŠ¶æ€"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä½¿ç”¨
        disk = psutil.disk_usage('/')
        
        # ç½‘ç»œIO
        network = psutil.net_io_counters()
        
        print(f"ğŸ“Š {timestamp} ç³»ç»ŸçŠ¶æ€:")
        print(f"  CPU: {cpu_percent}%")
        print(f"  å†…å­˜: {memory.percent}% ({memory.used/1024/1024/1024:.1f}GB/{memory.total/1024/1024/1024:.1f}GB)")
        print(f"  ç£ç›˜: {disk.percent}% ({disk.used/1024/1024/1024:.1f}GB/{disk.total/1024/1024/1024:.1f}GB)")
        print(f"  ç½‘ç»œ: â†‘{network.bytes_sent/1024/1024:.1f}MB â†“{network.bytes_recv/1024/1024:.1f}MB")

# ä½¿ç”¨ç¤ºä¾‹
monitor = SystemMonitor(interval=30)
monitor.start()

# è¿è¡Œä½ çš„ä»£ç ...

monitor.stop()
```

---

é€šè¿‡ä½¿ç”¨è¿™äº›æ•…éšœæ’é™¤å·¥å…·å’Œè§£å†³æ–¹æ¡ˆï¼Œæ‚¨å¯ä»¥å¿«é€Ÿè¯Šæ–­å’Œè§£å†³Gold-Seekerå¹³å°è¿è¡Œä¸­é‡åˆ°çš„å„ç§é—®é¢˜ã€‚è®°ä½ï¼Œè‰¯å¥½çš„æ—¥å¿—è®°å½•å’Œç›‘æ§æ˜¯é¢„é˜²é—®é¢˜çš„å…³é”®ã€‚