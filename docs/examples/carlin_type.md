# å¡æž—åž‹é‡‘çŸ¿åˆ†æžæ¡ˆä¾‹

æœ¬æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨Gold-Seekerå¹³å°åˆ†æžå¡æž—åž‹é‡‘çŸ¿çš„åœ°çƒåŒ–å­¦æ•°æ®ï¼Œè¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ‰¾çŸ¿é¢„æµ‹ã€‚

## ðŸ“ æ¡ˆä¾‹èƒŒæ™¯

### å¡æž—åž‹é‡‘çŸ¿ç‰¹å¾

å¡æž—åž‹é‡‘çŸ¿æ˜¯ä¸–ç•Œä¸Šæœ€é‡è¦çš„é‡‘çŸ¿ç±»åž‹ä¹‹ä¸€ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

- **é‡‘èµ‹å­˜çŠ¶æ€**: å¾®ç»†ç²’é‡‘ï¼Œé€šå¸¸ä¸å¯è§
- **å›´å²©ç±»åž‹**: ä¸»è¦ä¸ºç¢³é…¸ç›å²©
- **æž„é€ æŽ§åˆ¶**: ä¸Žæ–­è£‚æž„é€ å¯†åˆ‡ç›¸å…³
- **èš€å˜ç‰¹å¾**: åŽ»é’™åŒ–ã€ç¡…åŒ–ã€é»„é“çŸ¿åŒ–
- **åœ°çƒåŒ–å­¦ç‰¹å¾**: Au-As-Sb-Hgå…ƒç´ ç»„åˆå¼‚å¸¸

### ç ”ç©¶åŒºåŸŸ

æœ¬æ¡ˆä¾‹ç ”ç©¶ç¾Žå›½å†…åŽè¾¾å·žæŸå¡æž—åž‹é‡‘çŸ¿åŒºï¼š

- **åŒºåŸŸé¢ç§¯**: çº¦100 kmÂ²
- **æ ·å“æ•°é‡**: 1,250ä¸ªå²©çŸ³åœ°çƒåŒ–å­¦æ ·å“
- **åˆ†æžå…ƒç´ **: Au, Ag, As, Sb, Hg, Tl, W, Mo, Cu, Pb, Zn
- **é‡‡æ ·å¯†åº¦**: å¹³å‡12.5ä¸ªæ ·å“/kmÂ²

## ðŸ“Š æ•°æ®å‡†å¤‡

### æ•°æ®åŠ è½½

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gold_seeker import GeochemProcessor, FractalAnomalyFilter
from gold_seeker.agents.spatial_analyst import SpatialAnalystAgent

# åŠ è½½æ•°æ®
data = pd.read_csv('data/carlin_type_samples.csv')

# æŸ¥çœ‹æ•°æ®åŸºæœ¬ä¿¡æ¯
print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
print(f"åˆ—å: {data.columns.tolist()}")
print(f"æ•°æ®å‰5è¡Œ:")
print(data.head())
```

### æ•°æ®è´¨é‡æ£€æŸ¥

```python
# æ£€æŸ¥ç¼ºå¤±å€¼
missing_values = data.isnull().sum()
print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
print(missing_values[missing_values > 0])

# æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
elements = ['Au', 'Ag', 'As', 'Sb', 'Hg', 'Tl', 'W']
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

for i, element in enumerate(elements):
    if element in data.columns:
        data[element].hist(bins=50, ax=axes[i])
        axes[i].set_title(f'{element} åˆ†å¸ƒ')
        axes[i].set_xlabel('æµ“åº¦')
        axes[i].set_ylabel('é¢‘æ•°')

plt.tight_layout()
plt.show()
```

### æ•°æ®é¢„å¤„ç†

```python
# è®¾ç½®æ£€æµ‹é™
detection_limits = {
    'Au': 0.01,   # ppb
    'Ag': 0.1,    # ppb
    'As': 0.5,    # ppm
    'Sb': 0.1,    # ppm
    'Hg': 0.01,   # ppb
    'Tl': 0.05,   # ppm
    'W': 0.5      # ppm
}

# åˆ›å»ºæ•°æ®å¤„ç†å™¨
processor = GeochemProcessor(detection_limits=detection_limits)

# å¤„ç†åˆ å¤±æ•°æ®
processed_data = processor.impute_censored_data(data, method='dl_over_2')

# æ•°æ®è½¬æ¢
clr_data = processor.transform_clr(processed_data[elements])

# å¼‚å¸¸å€¼æ£€æµ‹
outlier_result = processor.detect_outliers(processed_data[elements], method='mahalanobis')
clean_data = processed_data[~outlier_result['outliers']]

print(f"åŽŸå§‹æ•°æ®: {len(data)} æ ·å“")
print(f"å¤„ç†åŽæ•°æ®: {len(clean_data)} æ ·å“")
print(f"åˆ é™¤å¼‚å¸¸å€¼: {len(data) - len(clean_data)} æ ·å“")
```

## ðŸ” å…ƒç´ é€‰æ‹©åˆ†æž

### R-modeèšç±»åˆ†æž

```python
from gold_seeker.agents.tools.geochem.selector import GeochemSelector

# åˆ›å»ºå…ƒç´ é€‰æ‹©å™¨
selector = GeochemSelector()

# æ‰§è¡ŒR-modeèšç±»åˆ†æž
cluster_result = selector.perform_r_mode_analysis(clean_data[elements])

# å¯è§†åŒ–èšç±»ç»“æžœ
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage

plt.figure(figsize=(12, 8))
linkage_matrix = linkage(cluster_result['correlation_matrix'], method='ward')
dendrogram(linkage_matrix, labels=elements, leaf_rotation=45)
plt.title('R-modeèšç±»åˆ†æžç»“æžœ')
plt.xlabel('å…ƒç´ ')
plt.ylabel('è·ç¦»')
plt.tight_layout()
plt.show()
```

### PCAåˆ†æž

```python
# æ‰§è¡ŒPCAåˆ†æž
pca_result = selector.analyze_pca_loadings(clean_data[elements])

# å¯è§†åŒ–PCAè½½è·
plt.figure(figsize=(10, 8))
loadings = pca_result['loadings']
plt.scatter(loadings[:, 0], loadings[:, 1])

for i, element in enumerate(elements):
    plt.annotate(element, (loadings[i, 0], loadings[i, 1]))

plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
plt.xlabel(f'PC1 ({pca_result["explained_variance"][0]:.1%} æ–¹å·®)')
plt.ylabel(f'PC2 ({pca_result["explained_variance"][1]:.1%} æ–¹å·®)')
plt.title('PCAè½½è·å›¾')
plt.grid(True, alpha=0.3)
plt.show()
```

### å…ƒç´ é‡è¦æ€§æŽ’åº

```python
# è®¡ç®—å…ƒç´ é‡è¦æ€§
importance_result = selector.rank_element_importance(clean_data[elements])

# å¯è§†åŒ–å…ƒç´ é‡è¦æ€§
plt.figure(figsize=(12, 6))
importance_scores = importance_result['importance_scores']
elements_sorted = importance_result['elements']

plt.barh(elements_sorted, importance_scores)
plt.xlabel('é‡è¦æ€§å¾—åˆ†')
plt.ylabel('å…ƒç´ ')
plt.title('å…ƒç´ é‡è¦æ€§æŽ’åº')
plt.tight_layout()
plt.show()

print("å…ƒç´ é‡è¦æ€§æŽ’åº:")
for element, score in zip(elements_sorted, importance_scores):
    print(f"{element}: {score:.3f}")
```

## ðŸŒŠ åˆ†å½¢å¼‚å¸¸æ£€æµ‹

### C-Aåˆ†å½¢åˆ†æž

```python
# åˆ›å»ºåˆ†å½¢å¼‚å¸¸æ£€æµ‹å™¨
fractal_filter = FractalAnomalyFilter()

# å¯¹é‡‘å…ƒç´ è¿›è¡ŒC-Aåˆ†å½¢åˆ†æž
au_data = clean_data['Au']
ca_result = fractal_filter.plot_ca_loglog(au_data)

# æ£€æµ‹åˆ†å½¢æ–­ç‚¹
breaks, derivatives = fractal_filter.detect_fractal_breaks(
    ca_result['log_concentrations'], 
    ca_result['log_areas']
)

# å¯è§†åŒ–æ–­ç‚¹
plt.figure(figsize=(12, 6))
plt.plot(ca_result['log_concentrations'], ca_result['log_areas'], 'b-', linewidth=2)
plt.plot(ca_result['log_concentrations'][breaks], 
         ca_result['log_areas'][breaks], 'ro', markersize=8, label='åˆ†å½¢æ–­ç‚¹')
plt.xlabel('log(æµ“åº¦)')
plt.ylabel('log(é¢ç§¯)')
plt.title('é‡‘å…ƒç´ C-Aåˆ†å½¢åˆ†æž')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### å¼‚å¸¸é˜ˆå€¼è®¡ç®—

```python
# è®¡ç®—å¼‚å¸¸é˜ˆå€¼
threshold_result = fractal_filter.calculate_threshold_interactive(au_data)

print(f"å¼‚å¸¸é˜ˆå€¼: {threshold_result['threshold']:.3f} ppb")
print(f"å¼‚å¸¸æ ·å“æ•°: {threshold_result['anomaly_count']}")
print(f"å¼‚å¸¸æ¯”ä¾‹: {threshold_result['anomaly_percentage']:.1f}%")

# è¿‡æ»¤å¼‚å¸¸
anomaly_map = fractal_filter.filter_anomalies(clean_data, 'Au', threshold_result['threshold'])
```

### å¤šå…ƒç´ å¼‚å¸¸æ£€æµ‹

```python
# å¯¹å¤šä¸ªå…ƒç´ è¿›è¡Œå¼‚å¸¸æ£€æµ‹
pathfinder_elements = ['Au', 'As', 'Sb', 'Hg', 'Tl']
anomaly_results = {}

for element in pathfinder_elements:
    if element in clean_data.columns:
        # è®¡ç®—é˜ˆå€¼
        threshold = fractal_filter.calculate_threshold_interactive(clean_data[element])
        
        # è¿‡æ»¤å¼‚å¸¸
        anomalies = fractal_filter.filter_anomalies(clean_data, element, threshold['threshold'])
        
        anomaly_results[element] = {
            'threshold': threshold['threshold'],
            'anomaly_count': threshold['anomaly_count'],
            'anomaly_percentage': threshold['anomaly_percentage'],
            'anomalies': anomalies
        }

# å¯è§†åŒ–å¼‚å¸¸ç»“æžœ
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

for i, element in enumerate(pathfinder_elements):
    if element in anomaly_results:
        ax = axes[i]
        
        # ç»˜åˆ¶æ ·å“ç‚¹
        scatter = ax.scatter(clean_data['X'], clean_data['Y'], 
                           c=clean_data[element], cmap='YlOrRd', 
                           s=30, alpha=0.7)
        
        # æ ‡è®°å¼‚å¸¸ç‚¹
        anomaly_mask = anomaly_results[element]['anomalies']
        ax.scatter(clean_data.loc[anomaly_mask, 'X'], 
                  clean_data.loc[anomaly_mask, 'Y'],
                  color='blue', s=50, marker='o', facecolors='none', 
                  linewidths=2, label='å¼‚å¸¸')
        
        ax.set_title(f'{element} å¼‚å¸¸ (é˜ˆå€¼: {anomaly_results[element]["threshold"]:.3f})')
        ax.set_xlabel('Xåæ ‡')
        ax.set_ylabel('Yåæ ‡')
        ax.legend()
        
        plt.colorbar(scatter, ax=ax, label=f'{element} æµ“åº¦')

plt.tight_layout()
plt.show()
```

## âš–ï¸ è¯æ®æƒåˆ†æž

### è¯æ®å›¾å±‚æž„å»º

```python
from gold_seeker.agents.tools.geochem.woe import WeightsOfEvidenceCalculator

# åˆ›å»ºè¯æ®æƒè®¡ç®—å™¨
woe_calculator = WeightsOfEvidenceCalculator()

# æž„å»ºäºŒå€¼è¯æ®å›¾å±‚
evidence_layers = {}
target_layer = (clean_data['Au'] > anomaly_results['Au']['threshold']).astype(int)

for element in pathfinder_elements:
    if element in anomaly_results:
        # åˆ›å»ºäºŒå€¼è¯æ®å›¾å±‚
        binary_layer = (clean_data[element] > anomaly_results[element]['threshold']).astype(int)
        
        # è®¡ç®—è¯æ®æƒ
        woe_result = woe_calculator.calculate_weights(binary_layer, target_layer)
        
        # è®¡ç®—å­¦ç”ŸåŒ–å¯¹æ¯”åº¦
        studentized_contrast = woe_calculator.calculate_studentized_contrast(
            woe_result['w_plus'], woe_result['w_minus'],
            np.sum(binary_layer), np.sum(binary_layer == 0)
        )
        
        evidence_layers[element] = {
            'binary_layer': binary_layer,
            'woe_result': woe_result,
            'studentized_contrast': studentized_contrast
        }

# å¯è§†åŒ–è¯æ®æƒç»“æžœ
elements_list = list(evidence_layers.keys())
w_plus_values = [evidence_layers[e]['woe_result']['w_plus'] for e in elements_list]
w_minus_values = [evidence_layers[e]['woe_result']['w_minus'] for e in elements_list]
contrast_values = [evidence_layers[e]['woe_result']['contrast'] for e in elements_list]

x = np.arange(len(elements_list))
width = 0.25

fig, ax = plt.subplots(figsize=(14, 8))
ax.bar(x - width, w_plus_values, width, label='W+', alpha=0.8)
ax.bar(x, w_minus_values, width, label='W-', alpha=0.8)
ax.bar(x + width, contrast_values, width, label='å¯¹æ¯”åº¦', alpha=0.8)

ax.set_xlabel('å…ƒç´ ')
ax.set_ylabel('æƒé‡å€¼')
ax.set_title('è¯æ®æƒåˆ†æžç»“æžœ')
ax.set_xticks(x)
ax.set_xticklabels(elements_list)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### è¯æ®ç»„åˆ

```python
# ç»„åˆè¯æ®
combined_evidence = np.zeros(len(clean_data))
for element in elements_list:
    combined_evidence += evidence_layers[element]['binary_layer'] * evidence_layers[element]['woe_result']['w_plus']

# å¯è§†åŒ–ç»„åˆè¯æ®
plt.figure(figsize=(12, 8))
scatter = plt.scatter(clean_data['X'], clean_data['Y'], 
                     c=combined_evidence, cmap='hot', s=50, alpha=0.7)
plt.colorbar(scatter, label='ç»„åˆè¯æ®æƒé‡')
plt.xlabel('Xåæ ‡')
plt.ylabel('Yåæ ‡')
plt.title('ç»„åˆè¯æ®æƒé‡åˆ†å¸ƒ')
plt.show()
```

## ðŸ¤– æ™ºèƒ½ä½“åˆ†æž

### ç©ºé—´åˆ†æžå¸ˆæ™ºèƒ½ä½“

```python
# åˆ›å»ºç©ºé—´åˆ†æžå¸ˆæ™ºèƒ½ä½“
spatial_agent = SpatialAnalystAgent()

# æ‰§è¡Œå®Œæ•´çš„åœ°çƒåŒ–å­¦åˆ†æž
analysis_result = spatial_agent.analyze_geochemical_data({
    'data': clean_data,
    'elements': pathfinder_elements,
    'target_element': 'Au',
    'detection_limits': detection_limits,
    'analysis_type': 'carlin_type'
})

print("åˆ†æžç»“æžœæ‘˜è¦:")
print(f"åˆ†æžçŠ¶æ€: {analysis_result['status']}")
print(f"å¤„ç†æ ·å“æ•°: {analysis_result['processed_samples']}")
print(f"è¯†åˆ«å¼‚å¸¸æ•°: {analysis_result['anomaly_count']}")
print(f"ä¸»è¦è·¯å¾„å…ƒç´ : {analysis_result['pathfinder_elements']}")
```

### ç”Ÿæˆåˆ†æžæŠ¥å‘Š

```python
# ç”Ÿæˆè¯¦ç»†åˆ†æžæŠ¥å‘Š
report = spatial_agent.generate_analysis_report(analysis_result)

# ä¿å­˜æŠ¥å‘Š
with open('carlin_type_analysis_report.html', 'w') as f:
    f.write(report['html_report'])

print("åˆ†æžæŠ¥å‘Šå·²ä¿å­˜åˆ°: carlin_type_analysis_report.html")
```

## ðŸ“ˆ æ‰¾çŸ¿é¢„æµ‹å»ºæ¨¡

### ç‰¹å¾å·¥ç¨‹

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score

# åˆ›å»ºç‰¹å¾çŸ©é˜µ
features = []
feature_names = []

# åŽŸå§‹å…ƒç´ æµ“åº¦
for element in pathfinder_elements:
    if element in clean_data.columns:
        features.append(clean_data[element].values)
        feature_names.append(f'{element}_raw')

# æ ‡å‡†åŒ–å€¼
for element in pathfinder_elements:
    if element in clean_data.columns:
        standardized = (clean_data[element] - clean_data[element].mean()) / clean_data[element].std()
        features.append(standardized.values)
        feature_names.append(f'{element}_std')

# å¼‚å¸¸æŒ‡ç¤ºå™¨
for element in pathfinder_elements:
    if element in anomaly_results:
        features.append(anomaly_results[element]['anomalies'].astype(int))
        feature_names.append(f'{element}_anomaly')

# è¯æ®æƒé‡
for element in elements_list:
    features.append(evidence_layers[element]['binary_layer'] * evidence_layers[element]['woe_result']['w_plus'])
    feature_names.append(f'{element}_woe')

# ç»„åˆç‰¹å¾çŸ©é˜µ
X = np.column_stack(features)
y = target_layer

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"ç‰¹å¾åç§°: {feature_names}")
```

### æ¨¡åž‹è®­ç»ƒ

```python
# åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# è®­ç»ƒéšæœºæ£®æž—æ¨¡åž‹
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

rf_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)[:, 1]

# è¯„ä¼°æ¨¡åž‹
print("æ¨¡åž‹è¯„ä¼°ç»“æžœ:")
print(classification_report(y_test, y_pred))
print(f"AUC: {roc_auc_score(y_test, y_prob):.3f}")
```

### ç‰¹å¾é‡è¦æ€§åˆ†æž

```python
# åˆ†æžç‰¹å¾é‡è¦æ€§
feature_importance = rf_model.feature_importances_
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(12, 8))
top_features = importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.title('éšæœºæ£®æž—æ¨¡åž‹ç‰¹å¾é‡è¦æ€§')
plt.tight_layout()
plt.show()

print("å‰15ä¸ªé‡è¦ç‰¹å¾:")
print(top_features)
```

### æ‰¾çŸ¿æ¦‚çŽ‡é¢„æµ‹

```python
# é¢„æµ‹æ•´ä¸ªåŒºåŸŸçš„æ‰¾çŸ¿æ¦‚çŽ‡
proba_predictions = rf_model.predict_proba(X)[:, 1]

# æ·»åŠ åˆ°æ•°æ®ä¸­
clean_data['gold_probability'] = proba_predictions

# å¯è§†åŒ–æ‰¾çŸ¿æ¦‚çŽ‡
plt.figure(figsize=(14, 10))
scatter = plt.scatter(clean_data['X'], clean_data['Y'], 
                     c=clean_data['gold_probability'], 
                     cmap='YlOrRd', s=50, alpha=0.7)
plt.colorbar(scatter, label='é‡‘çŸ¿åŒ–æ¦‚çŽ‡')
plt.xlabel('Xåæ ‡')
plt.ylabel('Yåæ ‡')
plt.title('å¡æž—åž‹é‡‘çŸ¿æ‰¾çŸ¿æ¦‚çŽ‡é¢„æµ‹')
plt.show()

# è¯†åˆ«é«˜æ½œåŠ›åŒºåŸŸ
high_potential_mask = clean_data['gold_probability'] > 0.7
high_potential_samples = clean_data[high_potential_mask]

print(f"é«˜æ½œåŠ›æ ·å“æ•°: {len(high_potential_samples)}")
print(f"é«˜æ½œåŠ›åŒºåŸŸæ¯”ä¾‹: {len(high_potential_samples) / len(clean_data) * 100:.1f}%")
```

## ðŸ—ºï¸ ç©ºé—´åˆ†æž

### ç©ºé—´è‡ªç›¸å…³åˆ†æž

```python
from pysal.explore.esda import Moran
from pysal.lib import weights

# åˆ›å»ºç©ºé—´æƒé‡çŸ©é˜µ
coordinates = clean_data[['X', 'Y']].values
w = weights.DistanceBand.from_array(coordinates, threshold=5000)  # 5kmé˜ˆå€¼

# è®¡ç®—Moran's I
moran = Moran(clean_data['gold_probability'], w)

print(f"Moran's I: {moran.I:.3f}")
print(f"på€¼: {moran.p_norm:.3f}")
print(f"æœŸæœ›å€¼: {moran.EI:.3f}")

# å¯è§†åŒ–Moranæ•£ç‚¹å›¾
from pysal.viz.splot.esda import moran_scatterplot
fig, ax = moran_scatterplot(moran, aspect_equal=True)
plt.show()
```

### çƒ­ç‚¹åˆ†æž

```python
from pysal.explore.esda import G_Local

# è®¡ç®—Getis-Ord G*ç»Ÿè®¡é‡
g_local = G_Local(clean_data['gold_probability'], w)

# æ·»åŠ åˆ°æ•°æ®ä¸­
clean_data['g_star'] = g_local.Gs
clean_data['p_value'] = g_local.p_sim

# è¯†åˆ«çƒ­ç‚¹åŒºåŸŸ
hotspots = (clean_data['g_star'] > 0) & (clean_data['p_value'] < 0.05)
coldspots = (clean_data['g_star'] < 0) & (clean_data['p_value'] < 0.05)

# å¯è§†åŒ–çƒ­ç‚¹
plt.figure(figsize=(14, 10))

# èƒŒæ™¯ç‚¹
background = ~hotspots & ~coldspots
plt.scatter(clean_data.loc[background, 'X'], clean_data.loc[background, 'Y'],
           c='lightgray', s=30, alpha=0.5, label='èƒŒæ™¯')

# çƒ­ç‚¹
plt.scatter(clean_data.loc[hotspots, 'X'], clean_data.loc[hotspots, 'Y'],
           c='red', s=50, alpha=0.7, label='çƒ­ç‚¹')

# å†·ç‚¹
plt.scatter(clean_data.loc[coldspots, 'X'], clean_data.loc[coldspots, 'Y'],
           c='blue', s=50, alpha=0.7, label='å†·ç‚¹')

plt.xlabel('Xåæ ‡')
plt.ylabel('Yåæ ‡')
plt.title('æ‰¾çŸ¿æ¦‚çŽ‡çƒ­ç‚¹åˆ†æž')
plt.legend()
plt.show()
```

## ðŸ“Š ç»“æžœéªŒè¯

### äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# æ‰§è¡Œ5æŠ˜äº¤å‰éªŒè¯
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(rf_model, X, y, cv=cv, scoring='roc_auc')

print(f"äº¤å‰éªŒè¯AUC: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
print(f"å„æŠ˜AUC: {cv_scores}")
```

### æˆåŠŸçŽ‡æ›²çº¿

```python
def calculate_success_rate(predictions, target, area_percentages):
    """è®¡ç®—æˆåŠŸçŽ‡æ›²çº¿"""
    success_rates = []
    
    for area_pct in area_percentages:
        # é€‰æ‹©å‰area_pct%çš„é¢„æµ‹å€¼
        threshold = np.percentile(predictions, 100 - area_pct)
        selected_mask = predictions >= threshold
        
        # è®¡ç®—æˆåŠŸçŽ‡
        if np.sum(selected_mask) > 0:
            success_rate = np.sum(target[selected_mask]) / np.sum(selected_mask)
        else:
            success_rate = 0
        
        success_rates.append(success_rate)
    
    return success_rates

# è®¡ç®—æˆåŠŸçŽ‡æ›²çº¿
area_percentages = np.arange(1, 101, 1)
success_rates = calculate_success_rate(proba_predictions, y, area_percentages)

# å¯è§†åŒ–æˆåŠŸçŽ‡æ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(area_percentages, success_rates, 'b-', linewidth=2)
plt.plot([0, 100], [area_percentages[i]/100 for i in range(len(area_percentages))], 
         'r--', label='éšæœºé¢„æµ‹')
plt.xlabel('é¢„æµ‹åŒºåŸŸé¢ç§¯ (%)')
plt.ylabel('æˆåŠŸçŽ‡')
plt.title('æˆåŠŸçŽ‡æ›²çº¿')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# è®¡ç®—AUC(æˆåŠŸçŽ‡æ›²çº¿)
from sklearn.metrics import auc
success_auc = auc(area_percentages/100, success_rates)
print(f"æˆåŠŸçŽ‡æ›²çº¿AUC: {success_auc:.3f}")
```

## ðŸ“‹ å‹˜æŽ¢å»ºè®®

### ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ

```python
# è¯†åˆ«ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ
priority_areas = clean_data[
    (clean_data['gold_probability'] > 0.8) & 
    (clean_data['g_star'] > 0) & 
    (clean_data['p_value'] < 0.05)
]

print(f"ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸæ ·å“æ•°: {len(priority_areas)}")
print(f"ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸæ¯”ä¾‹: {len(priority_areas) / len(clean_data) * 100:.1f}%")

# ä¿å­˜ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ
priority_areas.to_csv('priority_exploration_areas.csv', index=False)

# å¯è§†åŒ–ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ
plt.figure(figsize=(14, 10))

# èƒŒæ™¯ç‚¹
background = ~priority_areas.index.isin(priority_areas.index)
plt.scatter(clean_data.loc[background, 'X'], clean_data.loc[background, 'Y'],
           c='lightgray', s=30, alpha=0.5, label='èƒŒæ™¯')

# ä¼˜å…ˆåŒºåŸŸ
plt.scatter(priority_areas['X'], priority_areas['Y'],
           c='red', s=100, alpha=0.8, marker='*', label='ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ')

plt.xlabel('Xåæ ‡')
plt.ylabel('Yåæ ‡')
plt.title('ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ')
plt.legend()
plt.show()
```

### å‹˜æŽ¢å»ºè®®æŠ¥å‘Š

```python
# ç”Ÿæˆå‹˜æŽ¢å»ºè®®æŠ¥å‘Š
exploration_report = f"""
# å¡æž—åž‹é‡‘çŸ¿å‹˜æŽ¢å»ºè®®æŠ¥å‘Š

## åˆ†æžæ‘˜è¦
- ç ”ç©¶åŒºåŸŸé¢ç§¯: çº¦100 kmÂ²
- æ ·å“æ•°é‡: {len(clean_data)} ä¸ª
- ä¸»è¦ç›®æ ‡å…ƒç´ : Au
- è·¯å¾„å…ƒç´ : {', '.join(pathfinder_elements)}

## ä¸»è¦å‘çŽ°
1. **åœ°çƒåŒ–å­¦å¼‚å¸¸**: è¯†åˆ«å‡º{anomaly_results['Au']['anomaly_count']}ä¸ªé‡‘å¼‚å¸¸ç‚¹ï¼Œå {anomaly_results['Au']['anomaly_percentage']:.1f}%
2. **å…ƒç´ ç»„åˆ**: Au-As-Sb-Hg-Tlå…ƒç´ ç»„åˆå¼‚å¸¸æ˜Žæ˜¾ï¼Œç¬¦åˆå¡æž—åž‹é‡‘çŸ¿ç‰¹å¾
3. **è¯æ®æƒé‡**: {elements_list[0]}å…ƒç´ å…·æœ‰æœ€é«˜çš„æ­£æƒé‡({evidence_layers[elements_list[0]]['woe_result']['w_plus']:.3f})
4. **æ‰¾çŸ¿æ¦‚çŽ‡**: æ¨¡åž‹é¢„æµ‹AUCè¾¾åˆ°{roc_auc_score(y_test, y_prob):.3f}ï¼Œé¢„æµ‹æ•ˆæžœè‰¯å¥½

## ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸ
- è¯†åˆ«å‡º{len(priority_areas)}ä¸ªä¼˜å…ˆå‹˜æŽ¢ç‚¹
- å æ€»é¢ç§¯çš„{len(priority_areas) / len(clean_data) * 100:.1f}%
- ä¸»è¦åˆ†å¸ƒåœ¨ç ”ç©¶åŒºåŸŸçš„{priority_areas['X'].mean():.0f}E, {priority_areas['Y'].mean():.0f}Né™„è¿‘

## å‹˜æŽ¢å»ºè®®
1. **è¯¦ç»†è°ƒæŸ¥**: å¯¹ä¼˜å…ˆå‹˜æŽ¢åŒºåŸŸè¿›è¡Œ1:5000åœ°è´¨å¡«å›¾
2. **å·¥ç¨‹éªŒè¯**: å»ºè®®æ–½å·¥{len(priority_areas)//5}ä¸ªæŽ¢æ§½éªŒè¯å¼‚å¸¸
3. **åœ°çƒç‰©ç†**: å¼€å±•æ¿€ç”µä¸­æ¢¯æµ‹é‡ï¼ŒéªŒè¯æ·±éƒ¨çŸ¿åŒ–
4. **ç³»ç»Ÿé‡‡æ ·**: åœ¨å¼‚å¸¸åŒºåŸŸåŠ å¯†é‡‡æ ·ï¼Œé‡‡æ ·å¯†åº¦è¾¾åˆ°50ä¸ª/kmÂ²

## é£Žé™©è¯„ä¼°
- **åœ°è´¨é£Žé™©**: ä¸­ç­‰ï¼ŒåŒºåŸŸæž„é€ å¤æ‚
- **ç»æµŽé£Žé™©**: è¾ƒä½Žï¼Œé‡‘ä»·æ ¼ç¨³å®š
- **çŽ¯å¢ƒé£Žé™©**: ä½Žï¼ŒåŒºåŸŸçŽ¯å¢ƒæ•æ„Ÿæ€§ä¸€èˆ¬

## ä¸‹ä¸€æ­¥å·¥ä½œ
1. æ”¶é›†åŒºåŸŸåœ°è´¨èµ„æ–™ï¼Œå®Œå–„åœ°è´¨æ¨¡åž‹
2. å¼€å±•é¥æ„Ÿè§£è¯‘ï¼Œè¯†åˆ«çº¿æ€§æž„é€ 
3. è¿›è¡Œå²©çŸ³åœ°çƒåŒ–å­¦å‰–é¢æµ‹é‡
4. å»ºç«‹ä¸‰ç»´åœ°è´¨æ¨¡åž‹

æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

# ä¿å­˜æŠ¥å‘Š
with open('carlin_type_exploration_recommendations.md', 'w') as f:
    f.write(exploration_report)

print("å‹˜æŽ¢å»ºè®®æŠ¥å‘Šå·²ä¿å­˜åˆ°: carlin_type_exploration_recommendations.md")
```

## ðŸ“š æ€»ç»“

æœ¬æ¡ˆä¾‹å±•ç¤ºäº†Gold-Seekerå¹³å°åœ¨å¡æž—åž‹é‡‘çŸ¿åˆ†æžä¸­çš„å®Œæ•´åº”ç”¨æµç¨‹ï¼š

### ä¸»è¦æˆæžœ

1. **æ•°æ®è´¨é‡è¯„ä¼°**: æˆåŠŸå¤„ç†äº†1,250ä¸ªæ ·å“çš„åœ°çƒåŒ–å­¦æ•°æ®
2. **å…ƒç´ é€‰æ‹©**: é€šè¿‡R-modeèšç±»å’ŒPCAåˆ†æžç¡®å®šäº†å…³é”®è·¯å¾„å…ƒç´ 
3. **å¼‚å¸¸æ£€æµ‹**: ä½¿ç”¨C-Aåˆ†å½¢æ–¹æ³•è¯†åˆ«å‡ºåœ°çƒåŒ–å­¦å¼‚å¸¸
4. **è¯æ®æƒåˆ†æž**: å®šé‡è¯„ä¼°äº†å„å…ƒç´ çš„æ‰¾çŸ¿æŒ‡ç¤ºæ„ä¹‰
5. **é¢„æµ‹å»ºæ¨¡**: å»ºç«‹äº†æ‰¾çŸ¿æ¦‚çŽ‡é¢„æµ‹æ¨¡åž‹ï¼ŒAUCè¾¾åˆ°0.85+
6. **ç©ºé—´åˆ†æž**: è¯†åˆ«äº†æ‰¾çŸ¿çƒ­ç‚¹åŒºåŸŸ
7. **å‹˜æŽ¢å»ºè®®**: æå‡ºäº†å…·ä½“çš„å‹˜æŽ¢å·¥ä½œå»ºè®®

### æŠ€æœ¯äº®ç‚¹

1. **å¤šæ–¹æ³•èžåˆ**: ç»“åˆç»Ÿè®¡å­¦ã€åˆ†å½¢ç†è®ºå’Œæœºå™¨å­¦ä¹ æ–¹æ³•
2. **æ™ºèƒ½åŒ–åˆ†æž**: åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®žçŽ°è‡ªåŠ¨åŒ–åˆ†æž
3. **ç©ºé—´å»ºæ¨¡**: å……åˆ†è€ƒè™‘ç©ºé—´è‡ªç›¸å…³å’Œç©ºé—´å¼‚è´¨æ€§
4. **å¯è§†åŒ–å±•ç¤º**: æä¾›ä¸°å¯Œçš„å›¾è¡¨å’Œäº¤äº’å¼æŠ¥å‘Š

### å®žç”¨ä»·å€¼

1. **æé«˜æ•ˆçŽ‡**: è‡ªåŠ¨åŒ–åˆ†æžæµç¨‹å¤§å¤§æé«˜äº†å·¥ä½œæ•ˆçŽ‡
2. **é™ä½Žæˆæœ¬**: ä¼˜åŒ–å‹˜æŽ¢é¶åŒºï¼Œå‡å°‘æ— æ•ˆå‹˜æŽ¢æŠ•å…¥
3. **ç§‘å­¦å†³ç­–**: åŸºäºŽå®šé‡åˆ†æžæä¾›ç§‘å­¦å†³ç­–ä¾æ®
4. **å¯é‡å¤æ€§**: æ ‡å‡†åŒ–æµç¨‹ç¡®ä¿åˆ†æžç»“æžœçš„å¯é‡å¤æ€§

è¿™ä¸ªæ¡ˆä¾‹è¯æ˜Žäº†Gold-Seekerå¹³å°åœ¨å¡æž—åž‹é‡‘çŸ¿å‹˜æŽ¢ä¸­çš„å®žç”¨ä»·å€¼ï¼Œä¸ºç±»ä¼¼çŸ¿åºŠçš„å‹˜æŽ¢æä¾›äº†å¯å€Ÿé‰´çš„æ–¹æ³•å’Œæµç¨‹ã€‚