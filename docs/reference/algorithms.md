# ç®—æ³•å‚è€ƒ

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»Gold-Seekerå¹³å°ä¸­ä½¿ç”¨çš„å„ç§ç®—æ³•å®ç°ï¼ŒåŒ…æ‹¬æ•°å­¦åŸç†ã€è®¡ç®—æ­¥éª¤å’Œä»£ç å®ç°ã€‚

## ğŸ“Š åœ°çƒåŒ–å­¦æ•°æ®å¤„ç†ç®—æ³•

### 1. åˆ å¤±æ•°æ®å¤„ç†ç®—æ³•

#### 1.1 æ£€æµ‹é™/2æ–¹æ³• (DL/2)

**æ•°å­¦åŸç†**:
å¯¹äºä½äºæ£€æµ‹é™çš„å€¼ï¼Œä½¿ç”¨æ£€æµ‹é™çš„ä¸€åŠè¿›è¡Œæ›¿æ¢ï¼š

$$x_{imputed} = \frac{DL}{2}$$

å…¶ä¸­ï¼š
- $x_{imputed}$: æ’è¡¥åçš„å€¼
- $DL$: æ£€æµ‹é™

**ä»£ç å®ç°**:
```python
def dl_over_2_imputation(data, detection_limits):
    """
    æ£€æµ‹é™/2æ–¹æ³•æ’è¡¥åˆ å¤±æ•°æ®
    
    å‚æ•°:
    - data: åœ°çƒåŒ–å­¦æ•°æ® (DataFrame)
    - detection_limits: æ£€æµ‹é™å­—å…¸
    
    è¿”å›:
    - imputed_data: æ’è¡¥åçš„æ•°æ®
    """
    imputed_data = data.copy()
    
    for element, dl in detection_limits.items():
        if element in data.columns:
            censored_mask = data[element] < dl
            imputed_data.loc[censored_mask, element] = dl / 2
    
    return imputed_data
```

#### 1.2 Rosneræ–¹æ³•

**æ•°å­¦åŸç†**:
åŸºäºæ­£æ€åˆ†å¸ƒå‡è®¾ï¼Œä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡è®¡ç®—åˆ å¤±æ•°æ®çš„æœŸæœ›å€¼ï¼š

$$E[X|X < DL] = \mu - \sigma \cdot \frac{\phi(z)}{\Phi(z)}$$

å…¶ä¸­ï¼š
- $\mu$: éåˆ å¤±æ•°æ®çš„å‡å€¼
- $\sigma$: éåˆ å¤±æ•°æ®çš„æ ‡å‡†å·®
- $z = \frac{DL - \mu}{\sigma}$: æ ‡å‡†åŒ–æ£€æµ‹é™
- $\phi(z)$: æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°
- $\Phi(z)$: æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°

**ä»£ç å®ç°**:
```python
import numpy as np
from scipy.stats import norm

def rosner_imputation(data, element, detection_limit):
    """
    Rosneræ–¹æ³•æ’è¡¥åˆ å¤±æ•°æ®
    
    å‚æ•°:
    - data: åœ°çƒåŒ–å­¦æ•°æ®
    - element: å…ƒç´ åç§°
    - detection_limit: æ£€æµ‹é™
    
    è¿”å›:
    - imputed_value: æ’è¡¥å€¼
    """
    # è·å–éåˆ å¤±æ•°æ®
    uncensored_data = data[data[element] >= detection_limit][element]
    
    if len(uncensored_data) == 0:
        return detection_limit / 2
    
    # ä¼°è®¡å‚æ•°
    mu = np.mean(uncensored_data)
    sigma = np.std(uncensored_data)
    
    # è®¡ç®—æ ‡å‡†åŒ–æ£€æµ‹é™
    z = (detection_limit - mu) / sigma
    
    # è®¡ç®—æœŸæœ›å€¼
    expected_value = mu - sigma * norm.pdf(z) / norm.cdf(z)
    
    return max(expected_value, detection_limit / 100)  # é¿å…è´Ÿå€¼æˆ–é›¶å€¼
```

#### 1.3 æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ–¹æ³•

**æ•°å­¦åŸç†**:
å¯¹äºåˆ å¤±æ•°æ®ï¼Œæ„å»ºä¼¼ç„¶å‡½æ•°ï¼š

$$L(\mu, \sigma) = \prod_{i \in U} f(x_i; \mu, \sigma) \cdot \prod_{j \in C} F(DL; \mu, \sigma)$$

å…¶ä¸­ï¼š
- $U$: éåˆ å¤±æ•°æ®é›†åˆ
- $C$: åˆ å¤±æ•°æ®é›†åˆ
- $f(x; \mu, \sigma)$: æ­£æ€åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å‡½æ•°
- $F(x; \mu, \sigma)$: æ­£æ€åˆ†å¸ƒç´¯ç§¯åˆ†å¸ƒå‡½æ•°

**ä»£ç å®ç°**:
```python
from scipy.optimize import minimize
from scipy.stats import norm

def maximum_likelihood_imputation(data, element, detection_limit):
    """
    æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ–¹æ³•æ’è¡¥åˆ å¤±æ•°æ®
    
    å‚æ•°:
    - data: åœ°çƒåŒ–å­¦æ•°æ®
    - element: å…ƒç´ åç§°
    - detection_limit: æ£€æµ‹é™
    
    è¿”å›:
    - imputed_value: æ’è¡¥å€¼
    """
    # åˆ†ç¦»åˆ å¤±å’Œéåˆ å¤±æ•°æ®
    uncensored = data[data[element] >= detection_limit][element]
    censored_count = len(data[data[element] < detection_limit])
    
    def negative_log_likelihood(params):
        mu, sigma = params
        
        # éåˆ å¤±æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶
        log_likelihood = np.sum(norm.logpdf(uncensored, mu, sigma))
        
        # åˆ å¤±æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶
        log_likelihood += censored_count * norm.logcdf(detection_limit, mu, sigma)
        
        return -log_likelihood
    
    # åˆå§‹å‚æ•°ä¼°è®¡
    initial_params = [np.mean(uncensored), np.std(uncensored)]
    
    # ä¼˜åŒ–
    result = minimize(negative_log_likelihood, initial_params, method='L-BFGS-B')
    
    if result.success:
        mu_opt, sigma_opt = result.x
        # è®¡ç®—åˆ å¤±æ•°æ®çš„æœŸæœ›å€¼
        z = (detection_limit - mu_opt) / sigma_opt
        expected_value = mu_opt - sigma_opt * norm.pdf(z) / norm.cdf(z)
        return max(expected_value, detection_limit / 100)
    else:
        # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œå›é€€åˆ°Rosneræ–¹æ³•
        return rosner_imputation(data, element, detection_limit)
```

### 2. æ•°æ®è½¬æ¢ç®—æ³•

#### 2.1 ä¸­å¿ƒå¯¹æ•°æ¯”è½¬æ¢ (CLR)

**æ•°å­¦åŸç†**:
å¯¹äºæˆåˆ†æ•°æ® $\mathbf{x} = (x_1, x_2, ..., x_D)$ï¼ŒCLRè½¬æ¢å®šä¹‰ä¸ºï¼š

$$clr(x_i) = \ln\left(\frac{x_i}{g(\mathbf{x})}\right)$$

å…¶ä¸­å‡ ä½•å‡å€¼ï¼š
$$g(\mathbf{x}) = \left(\prod_{j=1}^{D} x_j\right)^{1/D}$$

**ä»£ç å®ç°**:
```python
import numpy as np

def clr_transform(data):
    """
    ä¸­å¿ƒå¯¹æ•°æ¯”è½¬æ¢
    
    å‚æ•°:
    - data: æˆåˆ†æ•°æ® (DataFrameæˆ–numpyæ•°ç»„)
    
    è¿”å›:
    - clr_data: CLRè½¬æ¢åçš„æ•°æ®
    """
    if isinstance(data, pd.DataFrame):
        data_array = data.values
    else:
        data_array = data
    
    # æ·»åŠ å°å¸¸æ•°é¿å…å¯¹æ•°é›¶
    epsilon = 1e-10
    data_array = data_array + epsilon
    
    # è®¡ç®—å‡ ä½•å‡å€¼
    geometric_mean = np.exp(np.mean(np.log(data_array), axis=1))
    
    # CLRè½¬æ¢
    clr_data = np.log(data_array / geometric_mean[:, np.newaxis])
    
    if isinstance(data, pd.DataFrame):
        return pd.DataFrame(clr_data, index=data.index, columns=data.columns)
    else:
        return clr_data

def clr_inverse_transform(clr_data):
    """
    CLRé€†è½¬æ¢
    
    å‚æ•°:
    - clr_data: CLRè½¬æ¢åçš„æ•°æ®
    
    è¿”å›:
    - original_data: åŸå§‹æˆåˆ†æ•°æ®
    """
    if isinstance(clr_data, pd.DataFrame):
        clr_array = clr_data.values
    else:
        clr_array = clr_data
    
    # é€†è½¬æ¢
    exp_clr = np.exp(clr_array)
    original_data = exp_clr / np.sum(exp_clr, axis=1, keepdims=True)
    
    if isinstance(clr_data, pd.DataFrame):
        return pd.DataFrame(original_data, index=clr_data.index, columns=clr_data.columns)
    else:
        return original_data
```

#### 2.2 åŠ æ³•å¯¹æ•°æ¯”è½¬æ¢ (ALR)

**æ•°å­¦åŸç†**:
é€‰æ‹©å‚è€ƒæˆåˆ† $x_D$ï¼ŒALRè½¬æ¢å®šä¹‰ä¸ºï¼š

$$alr(x_i) = \ln\left(\frac{x_i}{x_D}\right), \quad i = 1, 2, ..., D-1$$

**ä»£ç å®ç°**:
```python
def alr_transform(data, reference_column):
    """
    åŠ æ³•å¯¹æ•°æ¯”è½¬æ¢
    
    å‚æ•°:
    - data: æˆåˆ†æ•°æ®
    - reference_column: å‚è€ƒåˆ—åæˆ–ç´¢å¼•
    
    è¿”å›:
    - alr_data: ALRè½¬æ¢åçš„æ•°æ®
    """
    if isinstance(data, pd.DataFrame):
        reference_data = data[reference_column].values
        other_columns = [col for col in data.columns if col != reference_column]
        other_data = data[other_columns].values
    else:
        reference_data = data[:, reference_column]
        other_data = np.delete(data, reference_column, axis=1)
    
    # æ·»åŠ å°å¸¸æ•°é¿å…å¯¹æ•°é›¶
    epsilon = 1e-10
    reference_data = reference_data + epsilon
    other_data = other_data + epsilon
    
    # ALRè½¬æ¢
    alr_data = np.log(other_data / reference_data[:, np.newaxis])
    
    if isinstance(data, pd.DataFrame):
        return pd.DataFrame(alr_data, index=data.index, columns=other_columns)
    else:
        return alr_data
```

#### 2.3 ç­‰è·å¯¹æ•°æ¯”è½¬æ¢ (ILR)

**æ•°å­¦åŸç†**:
ä½¿ç”¨æ­£äº¤åŸºå‘é‡è¿›è¡Œè½¬æ¢ï¼š

$$ilr(\mathbf{x}) = \mathbf{V}^T \cdot clr(\mathbf{x})$$

å…¶ä¸­ $\mathbf{V}$ æ˜¯æ­£äº¤çŸ©é˜µã€‚

**ä»£ç å®ç°**:
```python
def ilr_transform(data):
    """
    ç­‰è·å¯¹æ•°æ¯”è½¬æ¢
    
    å‚æ•°:
    - data: æˆåˆ†æ•°æ®
    
    è¿”å›:
    - ilr_data: ILRè½¬æ¢åçš„æ•°æ®
    """
    from scipy.linalg import orth
    
    # é¦–å…ˆè¿›è¡ŒCLRè½¬æ¢
    clr_data = clr_transform(data)
    
    # åˆ›å»ºæ­£äº¤åŸº
    if isinstance(clr_data, pd.DataFrame):
        n_components = clr_data.shape[1]
        V = orth(np.random.randn(n_components, n_components - 1))
    else:
        n_components = clr_data.shape[1]
        V = orth(np.random.randn(n_components, n_components - 1))
    
    # ILRè½¬æ¢
    ilr_data = clr_data @ V
    
    if isinstance(data, pd.DataFrame):
        column_names = [f'ILR_{i+1}' for i in range(ilr_data.shape[1])]
        return pd.DataFrame(ilr_data, index=data.index, columns=column_names)
    else:
        return ilr_data
```

## ğŸŒŠ åˆ†å½¢å¼‚å¸¸æ£€æµ‹ç®—æ³•

### 1. C-Aåˆ†å½¢æ¨¡å‹

#### 1.1 æµ“åº¦-é¢ç§¯å…³ç³»è®¡ç®—

**æ•°å­¦åŸç†**:
å¯¹äºç»™å®šæµ“åº¦é˜ˆå€¼ $c$ï¼Œè®¡ç®—æµ“åº¦å¤§äºç­‰äº $c$ çš„é¢ç§¯ $A(c)$ï¼š

$$A(c) = \text{Area}\{x \in \Omega | C(x) \geq c\}$$

**ä»£ç å®ç°**:
```python
import numpy as np
import matplotlib.pyplot as plt

def calculate_ca_relationship(data, n_bins=100):
    """
    è®¡ç®—æµ“åº¦-é¢ç§¯å…³ç³»
    
    å‚æ•°:
    - data: åœ°çƒåŒ–å­¦æ•°æ®
    - n_bins: æµ“åº¦åŒºé—´æ•°é‡
    
    è¿”å›:
    - concentrations: æµ“åº¦æ•°ç»„
    - areas: é¢ç§¯æ•°ç»„
    """
    # åˆ›å»ºæµ“åº¦åŒºé—´
    concentrations = np.linspace(data.min(), data.max(), n_bins)
    areas = []
    
    for c in concentrations:
        # è®¡ç®—æµ“åº¦å¤§äºç­‰äºcçš„é¢ç§¯ï¼ˆæ ·å“æ•°ï¼‰
        area = np.sum(data >= c)
        areas.append(area)
    
    return concentrations, np.array(areas)

def plot_ca_loglog(concentrations, areas, title="C-Aåˆ†å½¢å…³ç³»"):
    """
    ç»˜åˆ¶C-AåŒå¯¹æ•°å›¾
    
    å‚æ•°:
    - concentrations: æµ“åº¦æ•°ç»„
    - areas: é¢ç§¯æ•°ç»„
    - title: å›¾è¡¨æ ‡é¢˜
    """
    # è¿‡æ»¤é›¶å€¼
    valid_mask = (concentrations > 0) & (areas > 0)
    log_c = np.log10(concentrations[valid_mask])
    log_a = np.log10(areas[valid_mask])
    
    plt.figure(figsize=(10, 6))
    plt.plot(log_c, log_a, 'b-', linewidth=2, label='C-Aå…³ç³»')
    plt.xlabel('log(æµ“åº¦)')
    plt.ylabel('log(é¢ç§¯)')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.show()
    
    return log_c, log_a
```

#### 1.2 åˆ†å½¢æ–­ç‚¹æ£€æµ‹

**æ•°å­¦åŸç†**:
ä½¿ç”¨ä¸€é˜¶å¯¼æ•°æ£€æµ‹C-Aæ›²çº¿çš„æ–­ç‚¹ï¼š

$$\frac{d\log A}{d\log C} = \frac{\Delta \log A}{\Delta \log C}$$

**ä»£ç å®ç°**:
```python
from scipy.signal import find_peaks
from scipy.stats import linregress

def detect_fractal_breaks(log_c, log_a, min_distance=5):
    """
    æ£€æµ‹åˆ†å½¢æ–­ç‚¹
    
    å‚æ•°:
    - log_c: å¯¹æ•°æµ“åº¦
    - log_a: å¯¹æ•°é¢ç§¯
    - min_distance: æœ€å°æ–­ç‚¹é—´è·
    
    è¿”å›:
    - break_points: æ–­ç‚¹ç´¢å¼•
    - derivatives: å¯¼æ•°æ•°ç»„
    - fractal_dimensions: å„æ®µåˆ†å½¢ç»´æ•°
    """
    # è®¡ç®—ä¸€é˜¶å¯¼æ•°
    derivatives = np.diff(log_a) / np.diff(log_c)
    
    # å¯»æ‰¾å¯¼æ•°çš„æå€¼ç‚¹ï¼ˆæ–­ç‚¹ï¼‰
    peaks, _ = find_peaks(-np.abs(derivatives), distance=min_distance)
    
    # è®¡ç®—å„æ®µçš„åˆ†å½¢ç»´æ•°
    fractal_dimensions = []
    
    # æ·»åŠ èµ·å§‹å’Œç»“æŸç‚¹
    all_points = [0] + list(peaks) + [len(log_c) - 1]
    
    for i in range(len(all_points) - 1):
        start_idx = all_points[i]
        end_idx = all_points[i + 1]
        
        if end_idx - start_idx > 1:  # è‡³å°‘éœ€è¦2ä¸ªç‚¹
            slope, _, _, _, _ = linregress(
                log_c[start_idx:end_idx], 
                log_a[start_idx:end_idx]
            )
            fractal_dimension = -slope
            fractal_dimensions.append(fractal_dimension)
        else:
            fractal_dimensions.append(None)
    
    return peaks, derivatives, fractal_dimensions, all_points
```

#### 1.3 é˜ˆå€¼è®¡ç®—æ–¹æ³•

##### 1.3.1 è†ç‚¹æ£€æµ‹æ³•

**æ•°å­¦åŸç†**:
å¯»æ‰¾C-Aæ›²çº¿çš„æœ€å¤§æ›²ç‡ç‚¹ï¼š

$$\kappa = \frac{|y''|}{(1 + y'^2)^{3/2}}$$

**ä»£ç å®ç°**:
```python
from scipy.signal import argrelextrema
from scipy.interpolate import interp1d

def knee_detection_threshold(concentrations, areas):
    """
    ä½¿ç”¨è†ç‚¹æ£€æµ‹æ³•è®¡ç®—é˜ˆå€¼
    
    å‚æ•°:
    - concentrations: æµ“åº¦æ•°ç»„
    - areas: é¢ç§¯æ•°ç»„
    
    è¿”å›:
    - threshold: å¼‚å¸¸é˜ˆå€¼
    - knee_point: è†ç‚¹ç´¢å¼•
    """
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_mask = (concentrations > 0) & (areas > 0)
    log_c = np.log10(concentrations[valid_mask])
    log_a = np.log10(areas[valid_mask])
    
    # è®¡ç®—æ›²ç‡
    first_derivative = np.gradient(log_a, log_c)
    second_derivative = np.gradient(first_derivative, log_c)
    
    curvature = np.abs(second_derivative) / (1 + first_derivative**2)**1.5
    
    # å¯»æ‰¾æœ€å¤§æ›²ç‡ç‚¹
    knee_point = np.argmax(curvature)
    
    # è®¡ç®—é˜ˆå€¼
    threshold = concentrations[valid_mask][knee_point]
    
    return threshold, knee_point
```

##### 1.3.2 K-meansèšç±»æ³•

**æ•°å­¦åŸç†**:
ä½¿ç”¨K-meanså°†C-Aæ•°æ®åˆ†ä¸ºä¸¤ç±»ï¼Œå¯»æ‰¾åˆ†ç±»è¾¹ç•Œã€‚

**ä»£ç å®ç°**:
```python
from sklearn.cluster import KMeans

def kmeans_threshold(concentrations, areas, n_clusters=2):
    """
    ä½¿ç”¨K-meansèšç±»æ³•è®¡ç®—é˜ˆå€¼
    
    å‚æ•°:
    - concentrations: æµ“åº¦æ•°ç»„
    - areas: é¢ç§¯æ•°ç»„
    - n_clusters: èšç±»æ•°é‡
    
    è¿”å›:
    - threshold: å¼‚å¸¸é˜ˆå€¼
    - labels: èšç±»æ ‡ç­¾
    """
    # å‡†å¤‡æ•°æ®
    valid_mask = (concentrations > 0) & (areas > 0)
    log_c = np.log10(concentrations[valid_mask])
    log_a = np.log10(areas[valid_mask])
    
    # ç»„åˆç‰¹å¾
    features = np.column_stack([log_c, log_a])
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(features)
    
    # æ‰¾åˆ°å¼‚å¸¸ç±»ï¼ˆé€šå¸¸æµ“åº¦è¾ƒé«˜ï¼‰
    cluster_centers = kmeans.cluster_centers_
    anomaly_cluster = np.argmax(cluster_centers[:, 0])  # æµ“åº¦æœ€é«˜çš„ç±»
    
    # è®¡ç®—é˜ˆå€¼ï¼ˆå¼‚å¸¸ç±»çš„æœ€å°æµ“åº¦ï¼‰
    anomaly_mask = labels == anomaly_cluster
    threshold_idx = np.where(valid_mask)[0][anomaly_mask].min()
    threshold = concentrations[threshold_idx]
    
    return threshold, labels
```

##### 1.3.3 åˆ†æ®µçº¿æ€§æ‹Ÿåˆæ³•

**æ•°å­¦åŸç†**:
å°†C-Aæ›²çº¿åˆ†ä¸ºä¸¤æ®µçº¿æ€§éƒ¨åˆ†ï¼Œå¯»æ‰¾æœ€ä¼˜åˆ†å‰²ç‚¹ã€‚

**ä»£ç å®ç°**:
```python
def piecewise_linear_threshold(concentrations, areas):
    """
    ä½¿ç”¨åˆ†æ®µçº¿æ€§æ‹Ÿåˆæ³•è®¡ç®—é˜ˆå€¼
    
    å‚æ•°:
    - concentrations: æµ“åº¦æ•°ç»„
    - areas: é¢ç§¯æ•°ç»„
    
    è¿”å›:
    - threshold: å¼‚å¸¸é˜ˆå€¼
    - break_point: æ–­ç‚¹ç´¢å¼•
    - r_squared: æ‹Ÿåˆä¼˜åº¦
    """
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_mask = (concentrations > 0) & (areas > 0)
    log_c = np.log10(concentrations[valid_mask])
    log_a = np.log10(areas[valid_mask])
    
    best_r_squared = -np.inf
    best_break_point = None
    best_threshold = None
    
    # å°è¯•ä¸åŒçš„æ–­ç‚¹ä½ç½®
    for break_point in range(10, len(log_c) - 10):
        # ç¬¬ä¸€æ®µæ‹Ÿåˆ
        slope1, intercept1, _, _, _ = linregress(
            log_c[:break_point], log_a[:break_point]
        )
        
        # ç¬¬äºŒæ®µæ‹Ÿåˆ
        slope2, intercept2, _, _, _ = linregress(
            log_c[break_point:], log_a[break_point:]
        )
        
        # è®¡ç®—é¢„æµ‹å€¼
        pred1 = slope1 * log_c[:break_point] + intercept1
        pred2 = slope2 * log_c[break_point:] + intercept2
        pred_all = np.concatenate([pred1, pred2])
        
        # è®¡ç®—RÂ²
        ss_res = np.sum((log_a - pred_all) ** 2)
        ss_tot = np.sum((log_a - np.mean(log_a)) ** 2)
        r_squared = 1 - (ss_res / ss_tot)
        
        if r_squared > best_r_squared:
            best_r_squared = r_squared
            best_break_point = break_point
            best_threshold = concentrations[valid_mask][break_point]
    
    return best_threshold, best_break_point, best_r_squared
```

## âš–ï¸ è¯æ®æƒç®—æ³•

### 1. åŸºç¡€è¯æ®æƒè®¡ç®—

#### 1.1 æƒé‡è®¡ç®—å…¬å¼

**æ•°å­¦åŸç†**:
å¯¹äºè¯æ®å›¾å±‚ $E$ å’Œç›®æ ‡å›¾å±‚ $T$ï¼š

$$W^+ = \ln\left(\frac{P(E|T)}{P(E|\bar{T})}\right) = \ln\left(\frac{N(E \cap T)/N(T)}{N(E \cap \bar{T})/N(\bar{T})}\right)$$

$$W^- = \ln\left(\frac{P(\bar{E}|T)}{P(\bar{E}|\bar{T})}\right) = \ln\left(\frac{N(\bar{E} \cap T)/N(T)}{N(\bar{E} \cap \bar{T})/N(\bar{T})}\right)$$

$$C = W^+ - W^-$$

å…¶ä¸­ï¼š
- $W^+$: è¯æ®å­˜åœ¨æ—¶çš„æƒé‡
- $W^-$: è¯æ®ä¸å­˜åœ¨æ—¶çš„æƒé‡
- $C$: å¯¹æ¯”åº¦
- $N(\cdot)$: å•å…ƒæ ¼æ•°é‡

**ä»£ç å®ç°**:
```python
import numpy as np
from scipy.stats import norm

def calculate_weights(evidence_map, target_map):
    """
    è®¡ç®—è¯æ®æƒ
    
    å‚æ•°:
    - evidence_map: è¯æ®å›¾å±‚ (äºŒå€¼æ•°ç»„)
    - target_map: ç›®æ ‡å›¾å±‚ (äºŒå€¼æ•°ç»„)
    
    è¿”å›:
    - weights: æƒé‡å­—å…¸
    """
    # è®¡ç®—å„ç§ç»Ÿè®¡é‡
    total_cells = np.prod(evidence_map.shape)
    target_cells = np.sum(target_map > 0)
    non_target_cells = total_cells - target_cells
    
    evidence_with_target = np.sum((evidence_map > 0) & (target_map > 0))
    evidence_without_target = np.sum((evidence_map > 0) & (target_map == 0))
    
    no_evidence_with_target = np.sum((evidence_map == 0) & (target_map > 0))
    no_evidence_without_target = np.sum((evidence_map == 0) & (target_map == 0))
    
    # è®¡ç®—æƒé‡
    w_plus = np.log((evidence_with_target / target_cells) / 
                   (evidence_without_target / non_target_cells))
    
    w_minus = np.log((no_evidence_with_target / target_cells) / 
                    (no_evidence_without_target / non_target_cells))
    
    contrast = w_plus - w_minus
    
    # è®¡ç®—ç½®ä¿¡åº¦
    s2_w_plus = (1 / evidence_with_target) + (1 / evidence_without_target)
    s2_w_minus = (1 / no_evidence_with_target) + (1 / no_evidence_without_target)
    s2_contrast = s2_w_plus + s2_w_minus
    
    studentized_contrast = contrast / np.sqrt(s2_contrast)
    
    return {
        'w_plus': w_plus,
        'w_minus': w_minus,
        'contrast': contrast,
        'studentized_contrast': studentized_contrast,
        's2_w_plus': s2_w_plus,
        's2_w_minus': s2_w_minus,
        's2_contrast': s2_contrast,
        'statistics': {
            'evidence_with_target': evidence_with_target,
            'evidence_without_target': evidence_without_target,
            'no_evidence_with_target': no_evidence_with_target,
            'no_evidence_without_target': no_evidence_without_target,
            'target_cells': target_cells,
            'non_target_cells': non_target_cells
        }
    }
```

#### 1.2 è¿ç»­è¯æ®æƒé‡è®¡ç®—

**æ•°å­¦åŸç†**:
å¯¹äºè¿ç»­è¯æ®ï¼Œä½¿ç”¨æ¨¡ç³Šéš¶å±åº¦å‡½æ•°è½¬æ¢ä¸ºæƒé‡ï¼š

$$W(x) = W^+ \cdot \mu(x) + W^- \cdot (1 - \mu(x))$$

å…¶ä¸­ $\mu(x)$ æ˜¯éš¶å±åº¦å‡½æ•°ã€‚

**ä»£ç å®ç°**:
```python
def calculate_continuous_weights(evidence_data, target_data, 
                                membership_function='linear'):
    """
    è®¡ç®—è¿ç»­è¯æ®æƒé‡
    
    å‚æ•°:
    - evidence_data: è¿ç»­è¯æ®æ•°æ®
    - target_data: ç›®æ ‡æ•°æ®
    - membership_function: éš¶å±åº¦å‡½æ•°ç±»å‹
    
    è¿”å›:
    - weights: æƒé‡å­—å…¸
    """
    # é¦–å…ˆäºŒå€¼åŒ–ä»¥è®¡ç®—åŸºç¡€æƒé‡
    threshold = np.percentile(evidence_data, 80)  # ä½¿ç”¨80%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
    binary_evidence = (evidence_data >= threshold).astype(int)
    
    # è®¡ç®—åŸºç¡€æƒé‡
    basic_weights = calculate_weights(binary_evidence, target_data)
    
    # è®¡ç®—éš¶å±åº¦
    if membership_function == 'linear':
        membership = linear_membership(evidence_data, evidence_data.min(), evidence_data.max())
    elif membership_function == 'sigmoid':
        membership = sigmoid_membership(evidence_data)
    elif membership_function == 'gaussian':
        membership = gaussian_membership(evidence_data)
    else:
        raise ValueError(f"æœªçŸ¥çš„éš¶å±åº¦å‡½æ•°: {membership_function}")
    
    # è®¡ç®—è¿ç»­æƒé‡
    continuous_weights = (basic_weights['w_plus'] * membership + 
                        basic_weights['w_minus'] * (1 - membership))
    
    return {
        'continuous_weights': continuous_weights,
        'membership': membership,
        'basic_weights': basic_weights
    }

def linear_membership(data, min_val, max_val):
    """çº¿æ€§éš¶å±åº¦å‡½æ•°"""
    return (data - min_val) / (max_val - min_val)

def sigmoid_membership(data, k=1, x0=0):
    """Så‹éš¶å±åº¦å‡½æ•°"""
    return 1 / (1 + np.exp(-k * (data - x0)))

def gaussian_membership(data, sigma=1):
    """é«˜æ–¯éš¶å±åº¦å‡½æ•°"""
    mean = np.mean(data)
    return np.exp(-0.5 * ((data - mean) / sigma) ** 2)
```

### 2. æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ

#### 2.1 å¡æ–¹æ£€éªŒ

**æ•°å­¦åŸç†**:
ä½¿ç”¨å¡æ–¹æ£€éªŒéªŒè¯è¯æ®é—´çš„æ¡ä»¶ç‹¬ç«‹æ€§ï¼š

$$\chi^2 = \sum_{i,j,k} \frac{(O_{ijk} - E_{ijk})^2}{E_{ijk}}$$

å…¶ä¸­ $O_{ijk}$ æ˜¯è§‚æµ‹é¢‘æ•°ï¼Œ$E_{ijk}$ æ˜¯æœŸæœ›é¢‘æ•°ã€‚

**ä»£ç å®ç°**:
```python
from scipy.stats import chi2_contingency

def test_conditional_independence(evidence1, evidence2, target):
    """
    æ£€éªŒæ¡ä»¶ç‹¬ç«‹æ€§
    
    å‚æ•°:
    - evidence1: ç¬¬ä¸€ä¸ªè¯æ®å›¾å±‚
    - evidence2: ç¬¬äºŒä¸ªè¯æ®å›¾å±‚
    - target: ç›®æ ‡å›¾å±‚
    
    è¿”å›:
    - test_result: æ£€éªŒç»“æœ
    """
    # åˆ›å»ºä¸‰ç»´åˆ—è”è¡¨
    contingency_table = np.zeros((2, 2, 2))
    
    for i in [0, 1]:  # evidence1
        for j in [0, 1]:  # evidence2
            for k in [0, 1]:  # target
                mask = (evidence1 == i) & (evidence2 == j) & (target == k)
                contingency_table[i, j, k] = np.sum(mask)
    
    # é‡å¡‘ä¸º2Dè¡¨æ ¼è¿›è¡Œå¡æ–¹æ£€éªŒ
    contingency_2d = contingency_table.reshape(4, 2)
    
    # æ‰§è¡Œå¡æ–¹æ£€éªŒ
    chi2, p_value, dof, expected = chi2_contingency(contingency_2d)
    
    return {
        'chi2': chi2,
        'p_value': p_value,
        'degrees_of_freedom': dof,
        'expected_frequencies': expected,
        'contingency_table': contingency_table,
        'independent': p_value > 0.05  # æ˜¾è‘—æ€§æ°´å¹³0.05
    }
```

#### 2.2 äº’ä¿¡æ¯æ£€éªŒ

**æ•°å­¦åŸç†**:
ä½¿ç”¨äº’ä¿¡æ¯åº¦é‡å˜é‡é—´çš„ä¾èµ–å…³ç³»ï¼š

$$I(X;Y) = \sum_{x,y} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right)$$

**ä»£ç å®ç°**:
```python
from sklearn.metrics import mutual_info_score

def mutual_information_test(evidence1, evidence2, target):
    """
    ä½¿ç”¨äº’ä¿¡æ¯æ£€éªŒæ¡ä»¶ç‹¬ç«‹æ€§
    
    å‚æ•°:
    - evidence1: ç¬¬ä¸€ä¸ªè¯æ®å›¾å±‚
    - evidence2: ç¬¬äºŒä¸ªè¯æ®å›¾å±‚
    - target: ç›®æ ‡å›¾å±‚
    
    è¿”å›:
    - mi_result: äº’ä¿¡æ¯ç»“æœ
    """
    # è®¡ç®—äº’ä¿¡æ¯
    mi_e1_e2 = mutual_info_score(evidence1, evidence2)
    mi_e1_target = mutual_info_score(evidence1, target)
    mi_e2_target = mutual_info_score(evidence2, target)
    
    # æ¡ä»¶äº’ä¿¡æ¯è¿‘ä¼¼
    conditional_mi = mi_e1_e2 - mi_e1_target - mi_e2_target
    
    return {
        'mi_e1_e2': mi_e1_e2,
        'mi_e1_target': mi_e1_target,
        'mi_e2_target': mi_e2_target,
        'conditional_mi': conditional_mi,
        'independent': conditional_mi < 0.1  # é˜ˆå€¼å¯è°ƒ
    }
```

## ğŸ¤– æœºå™¨å­¦ä¹ ç®—æ³•

### 1. éšæœºæ£®æ—ç®—æ³•

#### 1.1 ç®—æ³•åŸç†

**æ•°å­¦åŸç†**:
éšæœºæ£®æ—é€šè¿‡é›†æˆå¤šä¸ªå†³ç­–æ ‘æ¥æé«˜é¢„æµ‹æ€§èƒ½ï¼š

$$\hat{f}(x) = \frac{1}{B} \sum_{b=1}^{B} T_b(x)$$

å…¶ä¸­ $T_b(x)$ æ˜¯ç¬¬ $b$ æ£µå†³ç­–æ ‘çš„é¢„æµ‹ã€‚

**ä»£ç å®ç°**:
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

class CustomRandomForest:
    """è‡ªå®šä¹‰éšæœºæ£®æ—å®ç°"""
    
    def __init__(self, n_estimators=100, max_depth=None, 
                 min_samples_split=2, min_samples_leaf=1,
                 max_features='sqrt', random_state=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.random_state = random_state
        self.trees = []
        self.feature_importances_ = None
    
    def fit(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—"""
        n_samples, n_features = X.shape
        self.trees = []
        self.feature_importances_ = np.zeros(n_features)
        
        # ç¡®å®šæ¯æ£µæ ‘çš„ç‰¹å¾æ•°é‡
        if self.max_features == 'sqrt':
            n_features_per_tree = int(np.sqrt(n_features))
        elif self.max_features == 'log2':
            n_features_per_tree = int(np.log2(n_features))
        else:
            n_features_per_tree = self.max_features
        
        np.random.seed(self.random_state)
        
        for _ in range(self.n_estimators):
            # Bootstrapé‡‡æ ·
            bootstrap_indices = np.random.choice(
                n_samples, n_samples, replace=True
            )
            X_bootstrap = X[bootstrap_indices]
            y_bootstrap = y[bootstrap_indices]
            
            # éšæœºé€‰æ‹©ç‰¹å¾
            feature_indices = np.random.choice(
                n_features, n_features_per_tree, replace=False
            )
            
            # è®­ç»ƒå†³ç­–æ ‘
            tree = DecisionTreeClassifier(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf,
                random_state=self.random_state
            )
            
            tree.fit(X_bootstrap[:, feature_indices], y_bootstrap)
            
            # ä¿å­˜æ ‘å’Œç‰¹å¾ç´¢å¼•
            self.trees.append({
                'tree': tree,
                'feature_indices': feature_indices
            })
            
            # ç´¯ç§¯ç‰¹å¾é‡è¦æ€§
            tree_importance = np.zeros(n_features)
            tree_importance[feature_indices] = tree.feature_importances_
            self.feature_importances_ += tree_importance
        
        # å¹³å‡ç‰¹å¾é‡è¦æ€§
        self.feature_importances_ /= self.n_estimators
        
        return self
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        predictions = []
        
        for tree_info in self.trees:
            tree = tree_info['tree']
            feature_indices = tree_info['feature_indices']
            
            tree_pred = tree.predict_proba(X[:, feature_indices])
            predictions.append(tree_pred)
        
        # å¹³å‡é¢„æµ‹
        avg_predictions = np.mean(predictions, axis=0)
        return avg_predictions
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return np.argmax(proba, axis=1)
```

### 2. æ”¯æŒå‘é‡æœºç®—æ³•

#### 2.1 ç®—æ³•åŸç†

**æ•°å­¦åŸç†**:
SVMé€šè¿‡å¯»æ‰¾æœ€ä¼˜è¶…å¹³é¢æ¥åˆ†ç±»æ•°æ®ï¼š

$$\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{n}\xi_i$$

çº¦æŸæ¡ä»¶ï¼š
$$y_i(w^T\phi(x_i) + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

**ä»£ç å®ç°**:
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

class SVMProspectivityModel:
    """SVMæ‰¾çŸ¿é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, kernel='rbf', C=1.0, gamma='scale', 
                 probability=True, random_state=None):
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.probability = probability
        self.random_state = random_state
        self.scaler = StandardScaler()
        self.svm = None
    
    def fit(self, X, y):
        """è®­ç»ƒSVMæ¨¡å‹"""
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ›å»ºSVMæ¨¡å‹
        self.svm = SVC(
            kernel=self.kernel,
            C=self.C,
            gamma=self.gamma,
            probability=self.probability,
            random_state=self.random_state
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.svm.fit(X_scaled, y)
        
        return self
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if self.svm is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self.scaler.transform(X)
        return self.svm.predict_proba(X_scaled)
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        if self.svm is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self.scaler.transform(X)
        return self.svm.predict(X_scaled)
    
    def get_support_vectors(self):
        """è·å–æ”¯æŒå‘é‡"""
        if self.svm is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        return self.svm.support_vectors_
    
    def decision_function(self, X):
        """å†³ç­–å‡½æ•°å€¼"""
        if self.svm is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self.scaler.transform(X)
        return self.svm.decision_function(X_scaled)
```

### 3. ç¥ç»ç½‘ç»œç®—æ³•

#### 3.1 å¤šå±‚æ„ŸçŸ¥æœº

**æ•°å­¦åŸç†**:
å‰å‘ä¼ æ’­ï¼š
$$z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$$
$$a^{(l)} = \sigma(z^{(l)})$$

åå‘ä¼ æ’­ï¼š
$$\delta^{(l)} = (W^{(l+1)})^T\delta^{(l+1)} \odot \sigma'(z^{(l)})$$

**ä»£ç å®ç°**:
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

class NeuralNetworkProspectivityModel:
    """ç¥ç»ç½‘ç»œæ‰¾çŸ¿é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, hidden_layers=[64, 32, 16], 
                 activation='relu', dropout_rate=0.3,
                 learning_rate=0.001, random_state=None):
        self.hidden_layers = hidden_layers
        self.activation = activation
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()
    
    def build_model(self, input_dim):
        """æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹"""
        tf.random.set_seed(self.random_state)
        
        model = Sequential()
        
        # è¾“å…¥å±‚
        model.add(Dense(self.hidden_layers[0], 
                       input_dim=input_dim, 
                       activation=self.activation))
        model.add(BatchNormalization())
        model.add(Dropout(self.dropout_rate))
        
        # éšè—å±‚
        for units in self.hidden_layers[1:]:
            model.add(Dense(units, activation=self.activation))
            model.add(BatchNormalization())
            model.add(Dropout(self.dropout_rate))
        
        # è¾“å‡ºå±‚
        model.add(Dense(1, activation='sigmoid'))
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=Adam(learning_rate=self.learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy', 'AUC']
        )
        
        return model
    
    def fit(self, X, y, validation_split=0.2, epochs=100, 
            batch_size=32, verbose=1):
        """è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹"""
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # æ„å»ºæ¨¡å‹
        self.model = self.build_model(X.shape[1])
        
        # æ—©åœå›è°ƒ
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_scaled, y,
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping],
            verbose=verbose
        )
        
        return history
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        
        # è¿”å›äºŒåˆ†ç±»æ¦‚ç‡
        return np.column_stack([1 - predictions, predictions])
    
    def predict(self, X, threshold=0.5):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba[:, 1] >= threshold).astype(int)
    
    def get_feature_importance(self, X, y):
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨æ’åˆ—é‡è¦æ€§ï¼‰"""
        from sklearn.inspection import permutation_importance
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœå°šæœªè®­ç»ƒï¼‰
        if self.model is None:
            self.fit(X, y, verbose=0)
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        X_scaled = self.scaler.transform(X)
        
        def model_predict(X):
            return self.model.predict(X).flatten()
        
        result = permutation_importance(
            model_predict, X_scaled, y,
            n_repeats=10, random_state=self.random_state
        )
        
        return result.importances_mean
```

## ğŸ“ˆ æ¨¡å‹è¯„ä¼°ç®—æ³•

### 1. äº¤å‰éªŒè¯ç®—æ³•

#### 1.1 ç©ºé—´äº¤å‰éªŒè¯

**æ•°å­¦åŸç†**:
ç©ºé—´äº¤å‰éªŒè¯è€ƒè™‘ç©ºé—´è‡ªç›¸å…³æ€§ï¼Œé¿å…ç©ºé—´è¿‡æ‹Ÿåˆï¼š

$$CV = \frac{1}{K}\sum_{k=1}^{K} \text{Score}(f_{-k}, D_k)$$

å…¶ä¸­ $f_{-k}$ æ˜¯åœ¨ç¬¬ $k$ æŠ˜ä¹‹å¤–çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚

**ä»£ç å®ç°**:
```python
from sklearn.model_selection import KFold
from sklearn.cluster import KMeans
import numpy as np

class SpatialCrossValidation:
    """ç©ºé—´äº¤å‰éªŒè¯"""
    
    def __init__(self, n_splits=5, spatial_cv=True, random_state=None):
        self.n_splits = n_splits
        self.spatial_cv = spatial_cv
        self.random_state = random_state
    
    def split(self, X, y, coordinates):
        """ç”Ÿæˆç©ºé—´äº¤å‰éªŒè¯åˆ†å‰²"""
        if self.spatial_cv:
            return self._spatial_split(coordinates)
        else:
            return self._random_split(len(X))
    
    def _spatial_split(self, coordinates):
        """ç©ºé—´åˆ†å‰²"""
        # ä½¿ç”¨K-meansèšç±»åˆ†å‰²ç©ºé—´
        kmeans = KMeans(n_clusters=self.n_splits, 
                       random_state=self.random_state)
        spatial_clusters = kmeans.fit_predict(coordinates)
        
        splits = []
        for i in range(self.n_splits):
            train_mask = spatial_clusters != i
            test_mask = spatial_clusters == i
            splits.append((train_mask, test_mask))
        
        return splits
    
    def _random_split(self, n_samples):
        """éšæœºåˆ†å‰²"""
        kf = KFold(n_splits=self.n_splits, 
                  shuffle=True, 
                  random_state=self.random_state)
        return kf.split(np.arange(n_samples))
    
    def evaluate(self, model, X, y, coordinates, scoring='roc_auc'):
        """è¯„ä¼°æ¨¡å‹"""
        from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
        
        scores = []
        splits = self.split(X, y, coordinates)
        
        for train_mask, test_mask in splits:
            # è®­ç»ƒæ¨¡å‹
            model.fit(X[train_mask], y[train_mask])
            
            # é¢„æµ‹
            if hasattr(model, 'predict_proba'):
                y_pred = model.predict_proba(X[test_mask])[:, 1]
            else:
                y_pred = model.predict(X[test_mask])
            
            # è®¡ç®—è¯„åˆ†
            if scoring == 'roc_auc':
                score = roc_auc_score(y[test_mask], y_pred)
            elif scoring == 'accuracy':
                y_pred_class = (y_pred > 0.5).astype(int)
                score = accuracy_score(y[test_mask], y_pred_class)
            elif scoring == 'f1':
                y_pred_class = (y_pred > 0.5).astype(int)
                score = f1_score(y[test_mask], y_pred_class)
            else:
                raise ValueError(f"æœªçŸ¥çš„è¯„åˆ†æŒ‡æ ‡: {scoring}")
            
            scores.append(score)
        
        return np.array(scores)
```

### 2. æˆåŠŸç‡æ›²çº¿ç®—æ³•

#### 2.1 æˆåŠŸç‡è®¡ç®—

**æ•°å­¦åŸç†**:
æˆåŠŸç‡æ›²çº¿è¡¡é‡é¢„æµ‹æ¨¡å‹åœ¨ä¸åŒé¢ç§¯æ¯”ä¾‹ä¸‹çš„é¢„æµ‹æ•ˆæœï¼š

$$SR(A) = \frac{\text{ç›®æ ‡åŒºåŸŸåœ¨é¢„æµ‹å‰}A\%\text{åŒºåŸŸä¸­çš„æ¯”ä¾‹}}{A\%}$$

**ä»£ç å®ç°**:
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import auc

class SuccessRateCurve:
    """æˆåŠŸç‡æ›²çº¿åˆ†æ"""
    
    def __init__(self):
        self.area_percentages = None
        self.success_rates = None
        self.auc_score = None
    
    def calculate(self, predictions, targets, area_percentages=None):
        """
        è®¡ç®—æˆåŠŸç‡æ›²çº¿
        
        å‚æ•°:
        - predictions: é¢„æµ‹æ¦‚ç‡
        - targets: çœŸå®æ ‡ç­¾
        - area_percentages: é¢ç§¯ç™¾åˆ†æ¯”æ•°ç»„
        
        è¿”å›:
        - area_percentages: é¢ç§¯ç™¾åˆ†æ¯”
        - success_rates: æˆåŠŸç‡
        - auc_score: AUCåˆ†æ•°
        """
        if area_percentages is None:
            area_percentages = np.arange(1, 101, 1)
        
        success_rates = []
        
        for area_pct in area_percentages:
            # é€‰æ‹©å‰area_pct%çš„é¢„æµ‹å€¼
            threshold = np.percentile(predictions, 100 - area_pct)
            selected_mask = predictions >= threshold
            
            # è®¡ç®—æˆåŠŸç‡
            if np.sum(selected_mask) > 0:
                success_rate = np.sum(targets[selected_mask]) / np.sum(selected_mask)
            else:
                success_rate = 0
            
            success_rates.append(success_rate)
        
        # è®¡ç®—AUC
        auc_score = auc(area_percentages / 100, success_rates)
        
        self.area_percentages = area_percentages
        self.success_rates = np.array(success_rates)
        self.auc_score = auc_score
        
        return area_percentages, np.array(success_rates), auc_score
    
    def plot(self, title="æˆåŠŸç‡æ›²çº¿"):
        """ç»˜åˆ¶æˆåŠŸç‡æ›²çº¿"""
        if self.area_percentages is None or self.success_rates is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨calculateæ–¹æ³•")
        
        plt.figure(figsize=(10, 6))
        plt.plot(self.area_percentages, self.success_rates, 
                'b-', linewidth=2, label=f'é¢„æµ‹æ¨¡å‹ (AUC={self.auc_score:.3f})')
        
        # éšæœºé¢„æµ‹åŸºçº¿
        random_line = self.area_percentages / 100
        plt.plot(self.area_percentages, random_line, 
                'r--', linewidth=2, label='éšæœºé¢„æµ‹')
        
        plt.xlabel('é¢„æµ‹åŒºåŸŸé¢ç§¯ (%)')
        plt.ylabel('æˆåŠŸç‡')
        plt.title(title)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xlim(0, 100)
        plt.ylim(0, 1)
        plt.show()
    
    def get_optimal_threshold(self, predictions, targets):
        """è·å–æœ€ä¼˜é˜ˆå€¼"""
        if self.area_percentages is None or self.success_rates is None:
            self.calculate(predictions, targets)
        
        # æ‰¾åˆ°æˆåŠŸç‡æœ€é«˜çš„ç‚¹
        optimal_idx = np.argmax(self.success_rates)
        optimal_area_pct = self.area_percentages[optimal_idx]
        optimal_threshold = np.percentile(predictions, 100 - optimal_area_pct)
        
        return {
            'threshold': optimal_threshold,
            'area_percentage': optimal_area_pct,
            'success_rate': self.success_rates[optimal_idx]
        }
```

## ğŸ“š æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†Gold-Seekerå¹³å°ä¸­ä½¿ç”¨çš„å„ç§ç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š

1. **åœ°çƒåŒ–å­¦æ•°æ®å¤„ç†**: åˆ å¤±æ•°æ®å¤„ç†ã€æ•°æ®è½¬æ¢ç®—æ³•
2. **åˆ†å½¢å¼‚å¸¸æ£€æµ‹**: C-Aåˆ†å½¢æ¨¡å‹ã€é˜ˆå€¼è®¡ç®—æ–¹æ³•
3. **è¯æ®æƒæ–¹æ³•**: æƒé‡è®¡ç®—ã€æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ
4. **æœºå™¨å­¦ä¹ **: éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œ
5. **æ¨¡å‹è¯„ä¼°**: äº¤å‰éªŒè¯ã€æˆåŠŸç‡æ›²çº¿

è¿™äº›ç®—æ³•æ„æˆäº†Gold-Seekerå¹³å°çš„æŠ€æœ¯åŸºç¡€ï¼Œä¸ºåœ°çƒåŒ–å­¦æ‰¾çŸ¿é¢„æµ‹æä¾›äº†ç§‘å­¦ã€å¯é çš„æ–¹æ³•æ”¯æ’‘ã€‚æ¯ç§ç®—æ³•éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡å’Œå®ç°ï¼Œç¡®ä¿è®¡ç®—ç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚